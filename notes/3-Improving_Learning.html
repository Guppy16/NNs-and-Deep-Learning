<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>improving_learning – NNs and DL</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-2fd201b856bd247fb79ba48b3f7323be.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-05013c58f41773649a02e0b95a32a956.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked nav-fixed quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const darkModeDefault = authorPrefersDark;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">NNs and DL</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://x.com/AkashGu30808281" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-twitter-x"></i></a>
    <a href="https://github.com/Guppy16/NNs-and-Deep-Learning" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/1-NNs_Intro.html">Notes</a></li><li class="breadcrumb-item"><a href="../notes/3-Improving_Learning.html">Improving Learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../README.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/VAE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">VAE</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/1-NNs_Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intro to Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/2-Backpropagation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Backpropagation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/3-Improving_Learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Improving Learning</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#improving-the-way-neural-networks-learn" id="toc-improving-the-way-neural-networks-learn" class="nav-link active" data-scroll-target="#improving-the-way-neural-networks-learn">Improving the way neural networks learn</a>
  <ul class="collapse">
  <li><a href="#the-cross-entropy-cost-function" id="toc-the-cross-entropy-cost-function" class="nav-link" data-scroll-target="#the-cross-entropy-cost-function">The cross-entropy cost function</a>
  <ul class="collapse">
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  <li><a href="#what-does-the-cross-entropy-mean-where-does-it-come-from" id="toc-what-does-the-cross-entropy-mean-where-does-it-come-from" class="nav-link" data-scroll-target="#what-does-the-cross-entropy-mean-where-does-it-come-from">What does the cross-entropy mean? Where does it come from?</a></li>
  </ul></li>
  <li><a href="#softmax" id="toc-softmax" class="nav-link" data-scroll-target="#softmax">Softmax</a>
  <ul class="collapse">
  <li><a href="#exercise" id="toc-exercise" class="nav-link" data-scroll-target="#exercise">Exercise</a></li>
  </ul></li>
  <li><a href="#overfitting-and-regularization" id="toc-overfitting-and-regularization" class="nav-link" data-scroll-target="#overfitting-and-regularization">Overfitting and regularization</a></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization">Regularization</a>
  <ul class="collapse">
  <li><a href="#weight-decay-or-l2-regularization" id="toc-weight-decay-or-l2-regularization" class="nav-link" data-scroll-target="#weight-decay-or-l2-regularization"><em>weight decay</em> or <em>L2 regularization</em></a></li>
  <li><a href="#l1-regularization" id="toc-l1-regularization" class="nav-link" data-scroll-target="#l1-regularization">L1 Regularization</a></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">Dropout</a></li>
  <li><a href="#artificially-increasing-the-training-set" id="toc-artificially-increasing-the-training-set" class="nav-link" data-scroll-target="#artificially-increasing-the-training-set">Artificially increasing the training set</a></li>
  <li><a href="#problem" id="toc-problem" class="nav-link" data-scroll-target="#problem">Problem</a></li>
  </ul></li>
  <li><a href="#weight-initialization" id="toc-weight-initialization" class="nav-link" data-scroll-target="#weight-initialization">Weight initialization</a></li>
  <li><a href="#how-to-choose-a-neural-networks-hyper-parameters" id="toc-how-to-choose-a-neural-networks-hyper-parameters" class="nav-link" data-scroll-target="#how-to-choose-a-neural-networks-hyper-parameters">How to choose a neural network’s hyper-parameters?</a>
  <ul class="collapse">
  <li><a href="#baseline" id="toc-baseline" class="nav-link" data-scroll-target="#baseline">Baseline</a></li>
  <li><a href="#choosing-hyperparemeters" id="toc-choosing-hyperparemeters" class="nav-link" data-scroll-target="#choosing-hyperparemeters">Choosing hyperparemeters</a></li>
  </ul></li>
  <li><a href="#other-techniques" id="toc-other-techniques" class="nav-link" data-scroll-target="#other-techniques">Other techniques</a>
  <ul class="collapse">
  <li><a href="#variations-on-gradient-descent" id="toc-variations-on-gradient-descent" class="nav-link" data-scroll-target="#variations-on-gradient-descent">Variations on Gradient descent</a></li>
  <li><a href="#other-models-of-artificial-neurons" id="toc-other-models-of-artificial-neurons" class="nav-link" data-scroll-target="#other-models-of-artificial-neurons">Other models of artificial neurons</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notes/1-NNs_Intro.html">Notes</a></li><li class="breadcrumb-item"><a href="../notes/3-Improving_Learning.html">Improving Learning</a></li></ol></nav></header>





<section id="improving-the-way-neural-networks-learn" class="level1">
<h1>Improving the way neural networks learn</h1>
<p>This markdown file contains my notes on Chapter 3 of the book <a href="http://neuralnetworksanddeeplearning.com/chap3.html">Neural Networks and Deep Learning</a>. This chapter will introduce many techniques to improve the network learning including:</p>
<ul>
<li>Cross Entropy cost function</li>
<li>Softmax on the output layer</li>
<li>4 Regularisation methods
<ul>
<li>L1 / L2 regularisation</li>
<li>dropout</li>
<li>artificial expansion of the training data</li>
</ul></li>
<li>Weight Initialisation</li>
<li>Heuristics for tuning Hyperparameters</li>
<li>Variations of Gradient Descent</li>
<li>Other activation funcitons: Tanh, ReLu</li>
</ul>
<section id="the-cross-entropy-cost-function" class="level2">
<h2 class="anchored" data-anchor-id="the-cross-entropy-cost-function">The cross-entropy cost function</h2>
<p>Motivation: As seen in Chap 2, a neuron learns very slowly if the activation function is saturated (i.e.&nbsp;<span class="math inline">\(z\)</span> is at an extreme such that the gradient <span class="math inline">\(\sigma'(z)\)</span> is too small to cause any substantial change in the update step). Consider the weights in the final layer:</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial C_{MSE}}{\partial w^L} &amp;= \delta^L {a^{L-1}}^T \\
&amp;= (a^L - y) \odot \sigma'(z^L) {a^{L-1}}^T \\
&amp; \propto \sigma'(z^L)
\end{align*}
\]</span></p>
<p>Cross-Entropy loss resolves this by removing the dependency on the gradient of the sigmoid activation function. The CE loss function <span class="math inline">\(C_{CE}\)</span> is given by:</p>
<p><span class="math display">\[
\begin{align*}
  C_{CE} &amp;= \frac{1}{n} \sum_x C_{CE,x} \\
  \text{where:}\\
  C_{CE,x} &amp;= - \sum_j \left[y_j \ln a_j^L + (1 - y_j) \ln(1 - a_j^L) \right] \\
  &amp;= -y \cdot \ln (a^L) - (1 - y) \cdot \ln (1 - a^L)
\end{align*}
\]</span></p>
<p>To find the gradient wrt to a weight</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial C_{CE,x}}{\partial w^L}
&amp;= \frac{\partial z^L}{\partial w^L} \frac{\partial a^L}{\partial z^L} \frac{\partial C_{CE,x}}{\partial a^L} \\
\text{Note } a^L = \sigma (z^L) \\
&amp;= ({a^{L-1}}^T \otimes I ) \ \sigma'(z^L) \cdot \left[ -y \cdot \frac{1}{\sigma (z^L)} +(1 - y) \cdot \frac{1}{(1 - \sigma(z^L))}\right] \\
&amp;= ({a^{L-1}}^T \otimes I ) \ \sigma'(z^L) \cdot \left[ \frac{\sigma(z^L) - y}{\sigma(z^L)(1 - \sigma(z^L))}\right] \\
\text{Note } \sigma' = \sigma(1 - \sigma) \\
&amp;= \left({a^{L-1}}^T \otimes I \right) \cdot \left( \sigma(z^L) - y \right) \\
&amp;= \left( \sigma(z^L) - y \right) {a^{L-1}}^T \\
\end{align*}
\]</span></p>
<p>i.e.&nbsp;the gradient is proprtional to the error - this makes much more sense, because now the network will learn quicker if the error is larger (which is much more human like as well). (Note: <a href="https://math.stackexchange.com/questions/1621948/derivative-of-a-vector-with-respect-to-a-matrix">vector-matrix derivative</a> and <a href="https://en.wikipedia.org/wiki/Vectorization_(mathematics)#Compatibility_with_Kronecker_products">Kronecher product</a>, though the last line is dubious.)</p>
<section id="exercises" class="level3">
<h3 class="anchored" data-anchor-id="exercises">Exercises</h3>
<blockquote class="blockquote">
<p>The right form of cross-entropy</p>
</blockquote>
<p>Consider the incorrect form; <span class="math inline">\(- [a \ln y + (1 - a) \ln (1 - y)]\)</span>. In a classification problem, <span class="math inline">\(y \in \{0,1\}\)</span> and in either case, the cost will be very large and positive: <span class="math inline">\(+\inf \times \{a, 1-a\}\)</span>.</p>
<blockquote class="blockquote">
<p>Regression problems</p>
</blockquote>
<p>In regression problems, <span class="math inline">\(y \in [0,1]\)</span>. The cross entropy will still be minimised when <span class="math inline">\(a(z) = y\)</span> by <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality">Gibb’s inequality</a>.</p>
<blockquote class="blockquote">
<p>Using the quadratic cost when we have linear neurons in the output layer</p>
</blockquote>
<p>Then we have:</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial C_{MSE,x}}{\partial w^L} &amp;= \delta^L {a^{L-1}}^T \\
&amp;= (a^L - y) {a^{L-1}}^T \\
\end{align*}
\]</span></p>
<p>This resolves the saturation issue in the final layer, however the issue still remains in the previous layers.</p>
</section>
<section id="what-does-the-cross-entropy-mean-where-does-it-come-from" class="level3">
<h3 class="anchored" data-anchor-id="what-does-the-cross-entropy-mean-where-does-it-come-from">What does the cross-entropy mean? Where does it come from?</h3>
<p>Working backwards, we can derive the CE loss by supposing that we want to satisfy this differential equation for the bias:</p>
<p><span class="math display">\[
\begin{align*}
  \frac{\partial C}{\partial b} &amp;= (a - y) \\
  \text{Using the chain rule} \\
  &amp;= \frac{\partial z}{\partial b} \frac{\partial a}{\partial z} \frac{\partial C}{\partial a} \\
  &amp;= I \Sigma'(z) \frac{\partial C}{\partial a} \\
  &amp;= \frac{\partial C}{\partial a} \odot \sigma'(z) = \frac{\partial C}{\partial a} \odot a(1 - a) \\
  \text{Equating the two} \\
  (a - y) &amp;= \frac{\partial C}{\partial a} \odot \sigma'(z) \\
  \text{Abusing division notation..} \\
  \frac{\partial C}{\partial a} &amp;= \frac{a - y}{a(1 - a)} \\
  \implies C
  &amp;= -\left[y \cdot \ln (a) + (1 - y) \cdot \ln (1 - a) \right] + \text{const} \\
  &amp;&amp;\text{as required} \\
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p>Another factor that may inhibit learning is the presence of the <span class="math inline">\(x_j\)</span> term in Equation (61). Because of this term, when an input <span class="math inline">\(x_j\)</span> is near to zero, the corresponding weight <span class="math inline">\(w_j\)</span> will learn slowly.</p>
</blockquote>
<ul>
<li>In the final layer of a multi-layer NN, <span class="math inline">\(x_j = a_j^{L-1}\)</span> (i.e.&nbsp;the activation of the neurons in the previous layer).</li>
<li>Equation 61 refers to: <span class="math inline">\(\frac{\partial C}{\partial w^L} = \delta^L {a^{L-1}}^T\)</span></li>
</ul>
<p>It is not possible to elimnate this term through a clever choice of cost function because of the <em>weighted input</em> equation: <span class="math inline">\(z = w \cdot a + b\)</span>. Because of the linearity in the derivative operator, <span class="math inline">\(a\)</span> will always appear as a factor when differentiating wrt <span class="math inline">\(w\)</span>.</p>
</section>
</section>
<section id="softmax" class="level2">
<h2 class="anchored" data-anchor-id="softmax">Softmax</h2>
<p>Instead of using sigmoid as the activation function in the output layer, we can use <em>softmax</em>:</p>
<p><span class="math display">\[
a_j^L = \text{softmax}(z, j) = \frac{\exp(z_j^L)}{\sum_k \exp(z_k^L)}
\]</span></p>
<ul>
<li>This can be interpreted as a probability distribution because it obeys the two laws:
<ul>
<li><span class="math inline">\(a_j^L &gt; 0\)</span> because <span class="math inline">\(\exp(z) &gt; 0\)</span></li>
<li><span class="math inline">\(\sum_j a_j^L = \sum_j \frac{\exp(z_j^L)}{\sum_k \exp(z_k^L)} = \frac{\sum_j \exp(z_j^L)}{\sum_k \exp(z_k^L)} = 1\)</span></li>
</ul></li>
</ul>
<p>Note: It is obvious that the output of a sigmoid layer will not form a probability distribution, because the outputs aren’t rescaled.</p>
<p>The associated cost function is the <em>log-likelihood</em> function:</p>
<p><span class="math display">\[
C_{LL,x} = -\ln (y^T a^L) \equiv - y^T \ln (a^L)
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the one-hot encoded class for the corresponing input <span class="math inline">\(x\)</span>.</p>
<section id="exercise" class="level3">
<h3 class="anchored" data-anchor-id="exercise">Exercise</h3>
<blockquote class="blockquote">
<p>Monoticity of softmax</p>
</blockquote>
<p>Consider the derivative of the softmax function wrt an input:</p>
<p><span class="math display">\[
\begin{align*}
  \frac{\partial a_j^L}{\partial z_k^L}
  &amp;= \frac{\partial}{\partial z_k^L} \left\{ \exp(z_j^L) \left(\sum_i \exp(z_i^L)\right)^{-1} \right\} \\
  %%% Consider j not equal to k
  \text{for } j \ne k \\
  &amp;= - \exp(z_j^L) \exp(z_k^L) \left(\sum_i \exp(z_i^L)\right)^{-2} \\
  &amp;= - a_j^L (1 - a_k^L)\\  
  &amp; &lt; 0 \\
  %%% Consider j=k
  \text{for } j = k \\
  &amp;= \exp(z_j^L) \left(\sum_i \exp(z_i^L)\right)^{-1} - \exp(2z_j^L) \left(\sum_i \exp(z_i^L)\right)^{-2} \\
  &amp;= a_j^L - {a_j^L}^2 = a_j^L (1 - a_j^L) \\
  &amp;= \exp(z_j^L) \left( \sum_{i \ne j} \exp(z_i^L) \right) \left(\sum_i \exp(z_i^L)\right)^{-2} \\
  % &amp;&gt; 0
\end{align*}
\]</span></p>
<p>Hence, increasing <span class="math inline">\(z_k^L\)</span> is guaranteed to increase the corresponding output activation <span class="math inline">\(a_k^L\)</span> while decreasing the others.</p>
<blockquote class="blockquote">
<p>Non-locality of softmax</p>
</blockquote>
<p>A consequence of the denominator of the softmax function is that the output depends on <em>all</em> the weighted inputs (unlike the sigmoid).</p>
<blockquote class="blockquote">
<p>Inverting the softmax layer</p>
</blockquote>
<p>To find the weighted input <span class="math inline">\(z_j^L\)</span> given the output activations <span class="math inline">\(\vec{a}_L\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
  a_j^L &amp;= \frac{\exp(z_j^L)}{\sum_k \exp(z_k^L)} \\
  \iff z_j^L &amp;= \ln(a_j^L) - \ln \left(\sum_k \exp(z_k^L)\right) \\
  &amp;= \ln(a_j^L) + C \ \text{, for some constant } C
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p>Avoiding learning slowdown</p>
</blockquote>
<p>Consider the derivative of the cost function wrt the weights:</p>
<p><span class="math display">\[
\begin{align*}
  \frac{\partial C_{LL,x}}{\partial w^L}
  &amp;= \frac{\partial z^L}{\partial w^L} \frac{\partial a^L}{\partial z^L} \frac{\partial C}{\partial a^L} \\
  &amp;= ({a^{L-1}}^T \otimes I) \frac{\partial a^L}{\partial z^L} \cdot - \frac{1}{a^L} \cdot y \\
  &amp;= \begin{cases}
    ({a^{L-1}}^T \otimes I)
  \end{cases}
\end{align*}
\]</span></p>
<p>TODO above..</p>
<blockquote class="blockquote">
<p>Where does the “softmax” name come from?</p>
</blockquote>
<p>Consider:</p>
<p><span class="math display">\[
\begin{align*}
  &amp;\lim_{c \to \inf} \frac{\exp cz_j}{\sum_k \exp c z_k} \\
  = &amp;\lim_{c \to \inf} \left(1 + \sum_{k \forall k \ne j} \exp c(z_k - z_j) \right)^{-1} \\
  \to &amp; \begin{cases}
    0 &amp; \text{if } z_j &lt; z_k \forall k \\
    1 &amp; \text{if } z_j &gt; z_k \forall k \\
    \frac{1}{m} &amp; \text{if } z_j = z_i \forall k : |k| = m
  \end{cases}
\end{align*}
\]</span></p>
<p>This is equivalent to the <em>argmax</em> function (if there are no <span class="math inline">\(z_k = z_j\)</span>).</p>
<blockquote class="blockquote">
<p>Backpropagation with softmax and the log-likelihood cost</p>
</blockquote>
<p>To find an expression for <span class="math inline">\(\delta^L = \frac{\partial C}{\partial z^L}\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
  \delta^L &amp;= \frac{\partial C}{\partial z^L} \\
  &amp;= \frac{\partial a^L}{\partial z^L} \frac{\partial C}{\partial a^L} \\
  &amp;= \frac{\partial a^L}{\partial z^L} \cdot - \frac{1}{a_L} \odot y \\
  \text{Note } y_j = 1 \\
  &amp;= \begin{cases}
    a_j^L - 1 &amp; \text{if } k = j \\
    1 - a_k^L &amp; \text{otherwise}\\
  \end{cases}
\end{align*}
\]</span></p>
<p>Slightly confused by the textbook.</p>
</section>
</section>
<section id="overfitting-and-regularization" class="level2">
<h2 class="anchored" data-anchor-id="overfitting-and-regularization">Overfitting and regularization</h2>
<ul>
<li><em>early stopping</em> is used to stop training when the classification accuracy of the validation set plateus.</li>
<li><em>hold out</em> method is when a validation set is used to tune the hyperparameters of the model.</li>
<li>Use more training data to prevent overfitting</li>
</ul>
</section>
<section id="regularization" class="level2">
<h2 class="anchored" data-anchor-id="regularization">Regularization</h2>
<section id="weight-decay-or-l2-regularization" class="level3">
<h3 class="anchored" data-anchor-id="weight-decay-or-l2-regularization"><em>weight decay</em> or <em>L2 regularization</em></h3>
<p><span class="math display">\[
C = C_0 +\frac{\lambda}{2n} \sum_w w^2
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the regularization parameter - larger <span class="math inline">\(\lambda\)</span> means larger weights will be penalised more. This leads to the following weight update equation:</p>
<p><span class="math display">\[
\begin{align*}
  \text{Gradient descent:}\\
  w &amp;\to \left(1 - \frac{\eta \lambda}{n} \right) w - \eta \frac{\partial C_0}{\partial w} \\
  \text{SGD:} \\
  w &amp;\to \left(1 - \frac{\eta \lambda}{n} \right) w - \frac{\eta}{m} \sum_m \frac{\partial C_{0,x}}{\partial w} \\
\end{align*}
\]</span></p>
<p>Hence, on each update, the weight is rescaled to be smaller, unless this is a detriment to the unregularized cost function. Note in SGD, the weight decay has the same factor of <span class="math inline">\(1/n\)</span>, but the unregularized cost function is averaged as normal.</p>
<ul>
<li>Note that the update equation for the biases doesn’t change, because the regularization term doesn’t include the biases! Whether or not to incldue biases is dependent on the network. Genereally it doesn’t affect the accuracy so much, however large biases can lead to saturation which may be desirable in some cases.</li>
<li>Note that when the training examples increases, the regularization patameter must be increased as well to keep <span class="math inline">\(\frac{\eta \lambda}{n}\)</span> the same</li>
<li>Without regularization, SGD can easily be stuck in a local minima. Intuitively, this can be because when the weight vectors are large, small changes to them will still point them in a similar direction to before - i.e.&nbsp;not all directions will be explored in the cost function landscape. Hence, regularization resolves this by keeping the weight vectors small, so that small changes will cause larger changes in direction, resulting in SGD not being stuck in local minima so often.</li>
</ul>
</section>
<section id="l1-regularization" class="level3">
<h3 class="anchored" data-anchor-id="l1-regularization">L1 Regularization</h3>
<p><span class="math display">\[
C = C_0 + \frac{\lambda}{n} \sum_w |w|
\]</span></p>
<p>This gives the update equation:</p>
<p><span class="math display">\[
w \to w' = w - \frac{\eta \lambda}{n} \text{sgn}(w) - \eta \frac{\partial C_0}{\partial w}
\]</span></p>
<p>Comparing this to L2 regularization, the differences are:</p>
<ul>
<li>When the weights are large, L2 will penalise more</li>
<li>When the weights are small, L1 will drive them to 0</li>
</ul>
<p>This gives the effect that L1 regularization will tend to have a relatively small number of high-importnace connections, while the other weigths are driven to 0.</p>
</section>
<section id="dropout" class="level3">
<h3 class="anchored" data-anchor-id="dropout">Dropout</h3>
<p>During the training procedure, we add extra steps:</p>
<ul>
<li>deactivate out half of the neurons in the hidden layer</li>
<li>perform fwd and backprop</li>
<li>reintroduce the neurons and repeat</li>
</ul>
<p>After training, all the nerons are kept active, but the weights in the connected layer are halved (because previously it was learning on only half as many neurons).</p>
<p>Heuristically, this works because dropout is analogous to ensembling multiple NNs. Another explanation is:</p>
<blockquote class="blockquote">
<p>“This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.”</p>
</blockquote>
</section>
<section id="artificially-increasing-the-training-set" class="level3">
<h3 class="anchored" data-anchor-id="artificially-increasing-the-training-set">Artificially increasing the training set</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../resources/figures/training-size-accuracy.png" class="img-fluid figure-img"></p>
<figcaption>Alt text</figcaption>
</figure>
</div>
<p>This graph shows that as the training set size increases, the accuracy increases. However gathering more data can be expensive. Instead, artificially increasing the training set size can make vast improvements:</p>
<ul>
<li>Rotating the images by some amount (if arbitrarily large rotations are allowed, then a 6 may look like a 9!)</li>
<li>“elastic distortions” aim to mimic the variance added by real human handwriting</li>
</ul>
<p>Note, the converse is also true: reducing the noise in the training data can be more beneficial in e.g.&nbsp;signal processing tasks such as speech recognition.</p>
<p>Key takeaway: Improving training data is just as important as improving the algorithms.</p>
</section>
<section id="problem" class="level3">
<h3 class="anchored" data-anchor-id="problem">Problem</h3>
<blockquote class="blockquote">
<p>How do our machine learning algorithms perform in the limit of very large data sets?</p>
</blockquote>
<p>Tbh, I’m not sure what is the correct extrapolation. Currently, LLMs continue to improve with more data. At some point, the data will be large enought that it encompasses all possibilties, hence the accuracy should reach 100% asymptotically (perhaps only asmyptotically, because one could construct an example that catches out the model?)</p>
</section>
</section>
<section id="weight-initialization" class="level2">
<h2 class="anchored" data-anchor-id="weight-initialization">Weight initialization</h2>
<p>Motivation: Consider intialising the weights and biases: <span class="math inline">\(w,b \sim \mathcal{N}(\mu=0, \sigma^2=1)\)</span>. Immediately after initialisation, the <em>weighted input</em> to neurons to any layer <span class="math inline">\(z^l = w^l \cdot a^{l-1} + b^l\)</span>, so <span class="math inline">\(z\)</span> will be the sum of Guassians which is also Guassian: <span class="math inline">\(z^l \sim \mathcal{N}(\mu=0, \sigma^2=n + 1)\)</span>, where <span class="math inline">\(n\)</span> is the number of non-zero activations in the previous layer (the <span class="math inline">\(+1\)</span> comes from the bias). Hence, <span class="math inline">\(z^l\)</span> has very large variance, which will make it more likely for that neuron to saturate and learn more slowly.</p>
<p>A better intialisation scheme is to set the variance of the weights: <span class="math inline">\(w \sim \mathcal{N}(0, 1/n_\text{in})\)</span>.</p>
<p>Interestingly, the initialisation of the bias doesn’t matter so much (as long as it doesn’t cause early saturation).</p>
<p>Note: weight initialisation improves the rate of convergence to an optimal set of weights and biases; however, it doesn’t necessarly improve the accuracy of the model. (in Chap 4, better initialisations <em>do</em> actually enable a other methods to improve the performance).</p>
<blockquote class="blockquote">
<p>Connecting regularization and the improved method of weight initialzation</p>
</blockquote>
<p>Recall: Weight update in stochastic gradient descent</p>
<p><span class="math display">\[
w \to w' = \left(1 - \frac{\eta \lambda}{n} \right) w - \frac{\eta}{m} \sum_m \frac{\partial C_{0,x}}{\partial w} \\
\]</span></p>
<p>Suppose we are using the original approach to weight initialization. A wavy argument can be formulated to show that L2 regularization leads to the improved initialization scheme:</p>
<ol type="1">
<li>In the first few epochs, <span class="math inline">\(\frac{\partial C}{\partial w}\)</span> will be small because the neurons are saturated. Hence, assuming <span class="math inline">\(\lambda\)</span> is large enough, the weight update equation simplifies to:</li>
</ol>
<p><span class="math display">\[
\begin{align*}
  \text{Per SGD update:} \\
  w \to w'&amp; = \left(1 - \frac{\eta \lambda}{n} \right) w \\
  \text{Per epoch:} \\
  w \to w'&amp; = \left(1 - \frac{\eta \lambda / m}{n/m} \right)^{\frac{n}{m}} w
\end{align*}
\]</span></p>
<ol start="2" type="1">
<li>For <span class="math inline">\(n/m \gg \eta \lambda / m\)</span>, the weight decay per epoch simplifies to: <span class="math inline">\(w' = \exp - (\eta \lambda / m)\)</span>.</li>
<li>(?? Not sure about this step) Supposing <span class="math inline">\(\lambda\)</span> is not too large, weight decays will tail off when the <span class="math inline">\(|w| \sim 1/\sqrt{n}\)</span></li>
</ol>
</section>
<section id="how-to-choose-a-neural-networks-hyper-parameters" class="level2">
<h2 class="anchored" data-anchor-id="how-to-choose-a-neural-networks-hyper-parameters">How to choose a neural network’s hyper-parameters?</h2>
<p>It can be overwhelming to choos hyperparemeters, because you don’t know where to start.</p>
<section id="baseline" class="level3">
<h3 class="anchored" data-anchor-id="baseline">Baseline</h3>
<p>As a baseline, aim to get <em>any non-trivial</em> learning by performing better than chance. The idea is to meaningfully reduce the problem so that you can feedback quick enough for you to iterate.</p>
<ul>
<li>Reduce the training set classes (e.g.&nbsp;0s and 1s only)
<ul>
<li>Speeds up training significantly</li>
<li>This can be useful for debugging as well</li>
<li>Note that changing the size of training set will require modification to the relevant hyper-parameters</li>
</ul></li>
<li>Reduce validation set size to speed up evaluation</li>
<li>Start off with no hidden layer</li>
</ul>
</section>
<section id="choosing-hyperparemeters" class="level3">
<h3 class="anchored" data-anchor-id="choosing-hyperparemeters">Choosing hyperparemeters</h3>
<ul>
<li>Pick the learning rate <span class="math inline">\(\eta\)</span> by monitoring the training cost.</li>
<li>Use <em>early stopping</em> to determine the number of epochs
<ul>
<li>However, in the early stages, don’t use early stopping, because increased epochs can help monitor regularization performance</li>
<li>A good rule of thumb is <em>no-improvement-in-ten-rule</em>, i.e.&nbsp;stop if no improvement after 10 epochs. This can be made more lenient as other parameters are well adjusted.</li>
<li>Other methods exist, which compromise achieving high validation accuracies for not training too long.</li>
</ul></li>
<li><em>Learning rate schedule</em> is used to vary the learning rate
<ul>
<li>One scheme is to hold the learning rate constant until the validation accuracy starts to worsen, in which case the learning rate should be decreased by a factor of 2 or 10.</li>
<li>Termination can be done when the learning rate has decrease by a factor of 100 or 1000.</li>
</ul></li>
<li>To determine the regularization parameter <span class="math inline">\(\lambda\)</span>, it is worth setting it to 0 and choosing the learning rate. Then set <span class="math inline">\(\lambda=1\)</span> and experiment</li>
<li>Use the validation accuracy to pick regularization hyperparemeter, mini-batch size, network parameters.</li>
</ul>
<blockquote class="blockquote">
<p>It’s tempting to use gradient descent to try to learn good values for hyper-paremeters</p>
</blockquote>
<p>Potentially this isn’t the best idea because the landscae isn’t broadly convex. Gradient descent is probably too slow for this. Alternative automated methods are:</p>
<ul>
<li><em>grid search</em></li>
<li><em>bayesian</em> approach</li>
</ul>
</section>
</section>
<section id="other-techniques" class="level2">
<h2 class="anchored" data-anchor-id="other-techniques">Other techniques</h2>
<section id="variations-on-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="variations-on-gradient-descent">Variations on Gradient descent</h3>
<section id="hessian-technique" class="level4">
<h4 class="anchored" data-anchor-id="hessian-technique">Hessian Technique</h4>
<p>Taylor expansion gives</p>
<p><span class="math display">\[
\begin{align*}
  C(w + \Delta w) &amp; \approx C(w) + \nabla C \cdot \Delta w + \frac{1}{2} \Delta w^T H \Delta w \\
  \text{Assuming H is +ve definite,} \\
  \text{The minimum is found as} \\
  \implies \Delta w &amp;= - H^{-1} \nabla C \\
  \text{Giving the update} \\
  w \to w' &amp;= w - H^{-1} \nabla C \\
\end{align*}
\]</span></p>
<p>Advantages:</p>
<ul>
<li>converges faster than 1st order approximation</li>
<li>versions of backprop exist to calculate the Hessian</li>
</ul>
<p>Disadvantage: Too expensive and space heavy</p>
</section>
<section id="momentum-based-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="momentum-based-gradient-descent">Momentum-based gradient descent</h4>
<p>The Hessian adds information about the way the gradient is changing. The momentum based methods aims to do this by augmenting with a velocity term:</p>
<p><span class="math display">\[
\begin{align*}
  v \to v' &amp;= \mu v - \eta \nabla C \\
  w \to w' &amp;= w + v' \\
\end{align*}
\]</span></p>
<p><span class="math inline">\(\mu\)</span> controls the amount of velocity in the system (<em>momentum co-efficient</em>, though intuitively it controls the friction). <span class="math inline">\(\mu = 1 \implies\)</span> there is no friction, hence the system will roll down a hill and may overshoot. For <span class="math inline">\(\mu &gt; 0 \implies\)</span> the friction is infinte, i.e.&nbsp;the system reduces to the original udpate equations.</p>
<p>Note: for <span class="math inline">\(\mu &gt; 1\)</span>, energy will be added to the system, which could result in instability. For <span class="math inline">\(\mu &lt; 0\)</span>, gradient may traverse uphill instead.</p>
</section>
<section id="other-approaches" class="level4">
<h4 class="anchored" data-anchor-id="other-approaches">Other approaches</h4>
<ul>
<li>Conjugate gradient descent</li>
<li>BFGS / L-BFGS (the latter being the limited memory implentation)</li>
<li>What about <a href="https://en.wikipedia.org/wiki/Barzilai-Borwein_method">Barzilai-Borwein</a>?</li>
<li>Promising: Nesterov’s accelerated gradient technique (though it may be outdated now)</li>
</ul>
</section>
</section>
<section id="other-models-of-artificial-neurons" class="level3">
<h3 class="anchored" data-anchor-id="other-models-of-artificial-neurons">Other models of artificial neurons</h3>
<section id="tanh" class="level4">
<h4 class="anchored" data-anchor-id="tanh">Tanh</h4>
<p><span class="math display">\[
\sigma(z) = \frac{1}{2}(1 + \tanh(z/2))
\]</span></p>
<p>Hence, it is simply a linear transformation. Note that the output of tanh is <span class="math inline">\(\in (-1, 1)\)</span>, which can make interpretation slightly different. <em>Potentially</em>, tanh neurons can be better for learning because the output activation can be any sign. Consider BP4: <span class="math inline">\(\partial C / \partial w = \delta^l {a^{l-1}}^T\)</span>; so for a sigmoid neuron, all the activations will be positive, which will mean that a whole row of weights <span class="math inline">\(w_j^l\)</span> will increase / decrease depending on the sign of <span class="math inline">\(\delta_j^l\)</span>, rather than increasing / decreasing independently of each other. Moreover, tanh is an odd function, hence the activations will on average be equally balanced between +ve and -ve.</p>
</section>
<section id="relu" class="level4">
<h4 class="anchored" data-anchor-id="relu">ReLu</h4>
<p><em>rectified linear unit</em></p>
<p><span class="math display">\[
\text{ReLu} = \max(0, z)
\]</span></p>
<p>ReLu doesn’t suffer from the saturation problem. On the flip side, when <span class="math inline">\(z &lt; 0\)</span>, the gradient is 0, so the neuron stops learning entirely.</p>
<p>Note: ReLu requires a different initialisation scheme. <a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">This article</a> is very helpful introduction.</p>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/Guppy16\.github\.io\/NNs-and-Deep-Learning\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>