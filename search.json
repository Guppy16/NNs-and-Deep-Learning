[
  {
    "objectID": "experiments/classifier/chap1-no_hidden_layer-MSE_loss.html",
    "href": "experiments/classifier/chap1-no_hidden_layer-MSE_loss.html",
    "title": "Chap 1: Experiment 2",
    "section": "",
    "text": "This notebook trains a NN with no hidden layer and outputs some metrics such as the train and validation set accuracies.\n\n# Setup logging\nimport logging\nfrom digit_classifier import base, DigitClassifier, DEVICE, Metrics, DigitClassifierConfig\nimport torch.nn as nn\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import cm\nimport matplotlib.animation as ani\nimport seaborn as sns\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nsns.set_theme()\n\n[digit_classifier] [INFO] Added console handler\n[digit_classifier] [INFO] Torch device: cpu\n\n\n\nbase() # Set seeds\n\n\n# Model parameters\nconfig = DigitClassifierConfig(\n    sizes=[784, 10],\n    learning_rate=1,\n  device=DEVICE,\n  loss = nn.MSELoss(reduction='mean'),\n  mini_batch = 10,\n)\n\nmodel_dir = Path(\"../resources/model/chap1/no_hidden_layer_mse/\")\nmetrics_dir = model_dir / 'metrics.pkl'"
  },
  {
    "objectID": "experiments/classifier/chap1-no_hidden_layer-MSE_loss.html#training-a-nn-with-no-hidden-layer",
    "href": "experiments/classifier/chap1-no_hidden_layer-MSE_loss.html#training-a-nn-with-no-hidden-layer",
    "title": "Chap 1: Experiment 2",
    "section": "",
    "text": "This notebook trains a NN with no hidden layer and outputs some metrics such as the train and validation set accuracies.\n\n# Setup logging\nimport logging\nfrom digit_classifier import base, DigitClassifier, DEVICE, Metrics, DigitClassifierConfig\nimport torch.nn as nn\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import cm\nimport matplotlib.animation as ani\nimport seaborn as sns\nimport numpy as np\nfrom pathlib import Path\nimport pickle\nsns.set_theme()\n\n[digit_classifier] [INFO] Added console handler\n[digit_classifier] [INFO] Torch device: cpu\n\n\n\nbase() # Set seeds\n\n\n# Model parameters\nconfig = DigitClassifierConfig(\n    sizes=[784, 10],\n    learning_rate=1,\n  device=DEVICE,\n  loss = nn.MSELoss(reduction='mean'),\n  mini_batch = 10,\n)\n\nmodel_dir = Path(\"../resources/model/chap1/no_hidden_layer_mse/\")\nmetrics_dir = model_dir / 'metrics.pkl'"
  },
  {
    "objectID": "experiments/classifier/chap1-no_hidden_layer-MSE_loss.html#train-model",
    "href": "experiments/classifier/chap1-no_hidden_layer-MSE_loss.html#train-model",
    "title": "Chap 1: Experiment 2",
    "section": "Train model",
    "text": "Train model\nNote: this section can be skipped if model has already been trained\n\n# Instantiate model for training\nmodel = DigitClassifier(config)\nmodel\n\n[digit_classifier] [INFO] Train size: 60000\n[digit_classifier] [INFO] Test size: 10000\n[digit_classifier] [INFO] Train set: 50000\n[digit_classifier] [INFO] Valid set: 10000\n\n\nDigitClassifier(\n  (act_fn): Sigmoid()\n  (linears): ModuleList(\n    (0): Linear(in_features=784, out_features=10, bias=True)\n  )\n  (loss_module): MSELoss()\n)\n\n\n\nepochs = 10\nmetrics = Metrics()\nmodel.train_loop(num_epochs=epochs, metrics=metrics)\n\n[digit_classifier] [INFO] Epoch: 0: 8939.0 / 10000\n[digit_classifier] [INFO] Epoch: 1: 9009.0 / 10000\n[digit_classifier] [INFO] Epoch: 2: 9047.0 / 10000\n[digit_classifier] [INFO] Epoch: 3: 9080.0 / 10000\n[digit_classifier] [INFO] Epoch: 4: 9096.0 / 10000\n[digit_classifier] [INFO] Epoch: 5: 9104.0 / 10000\n[digit_classifier] [INFO] Epoch: 6: 9091.0 / 10000\n[digit_classifier] [INFO] Epoch: 7: 9097.0 / 10000\n[digit_classifier] [INFO] Epoch: 8: 9111.0 / 10000\n[digit_classifier] [INFO] Epoch: 9: 9120.0 / 10000\n\n\n\n# Save model\nmodel.save_model(model_dir)\n\n# Save metrics\nwith open(metrics_dir, 'wb')  as f:\n  pickle.dump(metrics, f)\n\nAs expected, a NN with no hidden layer performs worse: - Slower learning - Maximum achieved precsision is reduced\nHowever, it’s performance is still much better than any analytical approach (excluding SVM)."
  },
  {
    "objectID": "experiments/classifier/chap1-no_hidden_layer-MSE_loss.html#load-data-from-checkpoint",
    "href": "experiments/classifier/chap1-no_hidden_layer-MSE_loss.html#load-data-from-checkpoint",
    "title": "Chap 1: Experiment 2",
    "section": "Load data from checkpoint",
    "text": "Load data from checkpoint\nIf model has been trained before, you can skip here\n\n# Load model\nmodel = DigitClassifier.load_model(model_dir)\n\n# Load metrics\nwith open(metrics_dir, 'rb') as f:\n  metrics: Metrics = pickle.load(f)\n\n[digit_classifier] [INFO] Train size: 60000\n[digit_classifier] [INFO] Test size: 10000\n[digit_classifier] [INFO] Train set: 50000\n[digit_classifier] [INFO] Valid set: 10000\n\n\n\n# Plot train loss (per batch)\nx_epochs = np.arange(len(metrics.train_batch_loss)) / len(model.train_dataloader)\nax = sns.scatterplot(x=x_epochs, y=metrics.train_batch_loss, s=2, label=\"Training loss\", legend=\"auto\")\n# sns.scatterplot(x = x_epochs, y=moving_average(train_loss_metrics, n=len(model.train_dataloader)))\nax.set_xlabel(\"Epoch\")\nax.set_ylabel(\"MSE Loss\")\nax.set_title(\"Training loss calculated for each mini batch\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot training and validation accuracy per epoch\nax = sns.lineplot(metrics.valid_precision_epoch,\n                     label=\"Validation Accuracy\")\nsns.lineplot(metrics.train_precision_epoch, label=\"Training Accuracy\")\nax.set_ylim(None, 1)\nax.set_xlabel(\"Epoch\")\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Validation set accuracy every epoch\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualise the weights in the first layer\ndef weights(mdl: DigitClassifier, layer:int=0):\n  \"\"\"Return the weights in the layers 0th-&gt;layer\"\"\"\n  img_size = (28,28)  \n  return mdl.linears[0].weight[layer].detach().reshape(*img_size).cpu().numpy()\n\ndigit = 0\n\nfig, ax = plt.subplots()\nax.set_title(f\"0th layer weights for digit: {digit}\")\nim = ax.imshow(weights(model, digit), cmap=mpl.colormaps['seismic'])\nfig.colorbar(im, ax=ax)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n## Setup a color map renormaliser such that w=0 is in the middle\n# Print out some bounds to see what works best\n\n# Max value:\nbound = abs(model.linears[0].weight.detach()).max().numpy()\nprint(f\"Max value: {bound}\")\n\n# 3 std from the mean\nbound = 3 * abs(model.linears[0].weight.detach()).std().numpy()\nprint(f\"3 std: {bound}\")\n\n# Set bound to 0.5 (this seemed sensible)\nbound = 0.5\nprint(f\"Setting bound to: {bound}\")\nnorm = cm.colors.Normalize(vmax=bound, vmin=-bound)\n\nMax value: 1.9481966495513916\n3 std: 0.5106891989707947\nSetting bound to: 0.5\n\n\n\nfig, ax = plt.subplots()\nim = ax.imshow(weights(model, 0),\n               cmap=mpl.colormaps['seismic'], norm=norm, \n              #  interpolation='kaiser'\n               )\n\n# TODO: Try contour map\n# cs = ax.contour(weights, colors='k')\n# ax.contour(weights, origin=\"upper\") # Set origin if not using imshow\n\nfig.colorbar(im, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Animate the weights\nfps = 2\nfig, ax = plt.subplots()\nfig.subplots_adjust(left=0, bottom=0.02, right=1, top=1 - 0.08, wspace=None, hspace=None)\n\nim = ax.imshow(weights(model, 0),\n               cmap=mpl.colormaps['seismic'], norm=norm, \n              #  interpolation='kaiser'\n               )\n# ax.set_title(\"First layer weights\")\n# ax.set_axis('off')\nax.set_axis_off()\nfig.colorbar(im, ax=ax)\n\n\ndef next_ani(i):\n  ax.set_title(f\"Mask for digit: {i}\", fontsize=20)\n  w = weights(model, i)\n  im.set_array(w)\n  return im,\n\n\nanim = ani.FuncAnimation(fig, next_ani, frames=10, interval=1000)\nanim.save('../resources/figures/digit_weights_mse.gif')\n\nWARNING:matplotlib.animation:MovieWriter ffmpeg unavailable; using Pillow instead.\nINFO:matplotlib.animation:Animation.save using &lt;class 'matplotlib.animation.PillowWriter'&gt;"
  },
  {
    "objectID": "notes/VAE.html",
    "href": "notes/VAE.html",
    "title": "VAEs",
    "section": "",
    "text": "Now we look at exploring the latent space of a VAE based on known classes. The MNIST dataset contains class (digit) labels for each image and we can use this to explore how the latent space is organised for each class. Our encoder encodes a single image, \\(x\\) into a latent space \\(z\\) as \\(q(z|x) \\sim \\mathcal{N}(\\mu_x, \\Lambda_x)\\). So the latent distribution for a class \\(C\\) can be written as:\n\\[\n\\begin{align*}\np(z|C)\n&= \\int p(z|x) p(x|C) dx &&\\text{[Marginalisation]} \\\\\n&\\approx \\frac{1}{|C|} \\sum_{x \\in C} p(z|x), \\quad x \\sim p(x|C) &&\\text{[Monte Carlo Estimate]}\\\\\n&= \\frac{1}{|C|} \\sum_{x \\in C} \\mathcal{N}(z;\\ \\mu_x, \\Lambda_x) &&\\text{[VAE Encoder]}\n\\end{align*}\n\\]\nNote that this is a mixture of Gaussians, where each Gaussian is the latent distribution for a single image. (This is a sum of distributions, which should not be confused with summing Gaussian variables, the latter of which would be a Gaussian with a larger variance).\nWe note that our prior on the latent space is \\(z \\sim \\mathcal{N}(0, I)\\) and we try to enforce this with the KL term in the loss. However, perhaps counterintuitively, this does not mean that the \\(z|C\\) must also be \\(\\mathcal{N}(0,1)\\). In fact, we would expect the distributions to be rather different so that the decoder can learn to generate the correct digit from the latent space, which is emposed by the reconstruction loss.\nPerhaps we may expect some automatic clustering due to the similarity of the digits like so:\n1 7\n\n2 5 3\n\n0 8 6 9 4\n\nExercise: Calculate the mean and covariance of the latent space for each digit class.\n\nThe mean is given as follows\n\\[\n\\begin{align*}\n\\mu_C &= \\int z p(z|C) dz \\\\\n&= \\int z \\frac{1}{|C|} \\sum_{x \\in C} \\mathcal{N}(z;\\ \\mu_x, \\Lambda_x) dz \\\\\n&= \\frac{1}{|C|} \\sum_{x \\in C} \\int z \\mathcal{N}(z;\\ \\mu_x, \\Lambda_x) dz \\\\\n&= \\frac{1}{|C|} \\sum_{x \\in C} \\mu_x,\n\\end{align*}\n\\]\nwhere the last line follows from the definition of the mean of a Gaussian. Intuitively, this is the average of the latent space for each image in the class.\nTo find the covariance, we use the definition \\(\\Sigma_C = \\mathbb{E}[zz^T] - \\mathbb{E}[z]^2\\).\n\\[\n\\begin{align*}\n\\mathbb{E}[zz^T] &= \\int zz^T p(z|C) dz \\\\\n&= \\int zz^T \\frac{1}{|C|} \\sum_{x \\in C} \\mathcal{N}(z;\\ \\mu_x, \\Lambda_x) dz \\\\\n&= \\frac{1}{|C|} \\sum_{x \\in C} \\int zz^T \\mathcal{N}(z;\\ \\mu_x, \\Lambda_x) dz \\\\\n&= \\frac{1}{|C|} \\sum_{x \\in C} \\ \\Lambda_x + \\mu_x \\mu_x^T,\n\\end{align*}\n\\]\nThus, the covariance is\n\\[\n\\begin{align*}\n\\Sigma_C &= \\mathbb{E}[zz^T] - \\mathbb{E}[z]^2 \\\\\n&= \\frac{1}{|C|} \\sum_{x \\in C} \\ \\Lambda_x + \\mu_x \\mu_x^T - \\left(\\frac{1}{|C|} \\sum_{x \\in C} \\mu_x\\right)^2.\n\\end{align*}\n\\]\n\n\nIn this section, we aim to generate samples \\(x \\sim p(x | \\mathcal{D})\\) within the distribution of our datasets \\(\\mathcal{D}\\). The methods involve various methods of sampling from the latent space and passing through the decoder.\n\nSample from the Prior (Naive)\n\n\\[\n\\begin{align*}\n  z \\sim \\mathcal{N}(0, I) \\\\\n\\end{align*}\n\\]\n\nSample from the dataset and then the latent space (Monte Carlo)\n\n\\[\n\\begin{align*}\n  x &\\sim p_d(x) \\\\\n  z &\\sim \\mathcal{N}(\\mu_x, \\Lambda_x)\n\\end{align*}\n\\]\n\npCN sampling (MCMC)\n\nThis is a bit more involved.\nWe only use the decoder\nMetropolis-Hastings algorithm:\n\nSample \\(z_0 \\sim \\mathcal{N}(0, I)\\)\nFor \\(i = 1, \\ldots, N\\):\n\nSample from proposal \\(z' \\sim \\mathcal{P}(z_i)\\)\nCalculate the acceptance threshold \\(\\bar\\alpha\\)\nSample \\(u \\sim \\mathcal{U}(0, 1)\\)\nIf \\(u &lt; \\bar\\alpha\\), then \\(z_{i+1} = z'\\), else \\(z_{i+1} = z_i\\)\n\n\nNote the following\n\nthe acceptance probability is \\(\\alpha = \\min(1, \\bar\\alpha)\\), but we do not need to calculate this explicitly as we can just compare \\(u\\) to \\(\\bar\\alpha\\)\nin step 2.4 we may reject a sample, but this does not mean that the chain is stuck at generating the same image - this is because the decoder is in fact sampling from \\(p_\\theta(x|z) = \\mathcal{N}(\\text{Decode}(x), I)\\). So the decoder will generate a different image for the same latent space.\n\nFor pre-conditioned Crank-Nicolson (pCN) sampling, the proposal, \\(\\mathcal{P}(z_i \\to z')\\), is given by\n\\[\nz' = \\sqrt{1 - \\beta^2} \\ z_i + \\beta \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I).\n\\]\nFor pCN, the acceptance threshold simplifies as follows:\n\\[\n\\bar\\alpha\n= \\frac{p(z'|\\mathcal{D}) \\mathcal{P}(z_i \\to z')}{p(z_i|\\mathcal{D}) \\mathcal{P}(z' \\to z_i)}  \n= \\frac{p(\\mathcal{D}|z')}{p(\\mathcal{D}|z_i)}\n\\]\nThis simplification works out due to the construction of the pCN proposal, which we prove later. We note that this simplification is a remarkable result, as it means that we only require the likelihood, \\(p(\\mathcal{D}|z)\\), to calculate the acceptance threshold, and do not deal with the prior or proposal distributions, which may otherwise result in a low acceptance rate.\nUnder our VAE model, the likelihood is given by our decoder:\n\\[\n\\begin{align*}\n  p(\\mathcal{D}|z)\n  &= \\prod_i p_\\theta(x^i|z)  \\\\\n  &\\propto \\exp\\left(-\\frac{1}{2} \\sum_i \\left\\|x^i - \\text{Decode}(z)\\right\\|_2^2\\right)\n  .\n\\end{align*}\n\\]\n\nExercise: Prove the pCN acceptance threshold simplification. (We ignore the measure theoretic details).\n\n\n\n\n\nVisualise training of latent space in 2D / 3D - does this have the same representation as the classification exercise?\n\nThere are three phases to the KL loss (which is very apparent for latent_dim=8). Does this reflect in the latent space?!:\n\nKLD starts high, due to random initialisation of the model.\nKLD drops almost to 0, as the model learns to encode as a Gaussian very easily\nKLD rises sharply to compensate for the high MSE reconstruction loss.\nKLD increases slowly to match the MSE loss.\n\nUse a CNN / UNet as the encoder / decoder\n\n\n\nSo far, we have been able to visualise the latent space of a VAE for 2D and 3D latent spaces. We require a method to do this for higher dimensional latent spaces. Here are some options\n\nPotential methods to visulaise high dim space\n\nFor each dim, use violin / box plot to show distribution\n\nIf comparing two different runs, then use a violin plot with a violin for each run\n\nPerhaps can wrap these around a polar plot\n\nWe can plot e.g. the volume and direction of the latents. For higher dims, we could graph the volumne and directions\nFind path that traverses all digits in latent space\n\n\n\n\nWe would also want to visualise the effect of beta more clearly\n\nPerhaps use plotly with a slider?\nIs there a method to determine the optimal beta?\nshow how sliders affect the reconstruction: vary the value of each latent from -3 to 3 (3 sigmas)",
    "crumbs": [
      "VAE"
    ]
  },
  {
    "objectID": "notes/VAE.html#sampling-from-the-latent-space",
    "href": "notes/VAE.html#sampling-from-the-latent-space",
    "title": "VAEs",
    "section": "",
    "text": "In this section, we aim to generate samples \\(x \\sim p(x | \\mathcal{D})\\) within the distribution of our datasets \\(\\mathcal{D}\\). The methods involve various methods of sampling from the latent space and passing through the decoder.\n\nSample from the Prior (Naive)\n\n\\[\n\\begin{align*}\n  z \\sim \\mathcal{N}(0, I) \\\\\n\\end{align*}\n\\]\n\nSample from the dataset and then the latent space (Monte Carlo)\n\n\\[\n\\begin{align*}\n  x &\\sim p_d(x) \\\\\n  z &\\sim \\mathcal{N}(\\mu_x, \\Lambda_x)\n\\end{align*}\n\\]\n\npCN sampling (MCMC)\n\nThis is a bit more involved.\nWe only use the decoder\nMetropolis-Hastings algorithm:\n\nSample \\(z_0 \\sim \\mathcal{N}(0, I)\\)\nFor \\(i = 1, \\ldots, N\\):\n\nSample from proposal \\(z' \\sim \\mathcal{P}(z_i)\\)\nCalculate the acceptance threshold \\(\\bar\\alpha\\)\nSample \\(u \\sim \\mathcal{U}(0, 1)\\)\nIf \\(u &lt; \\bar\\alpha\\), then \\(z_{i+1} = z'\\), else \\(z_{i+1} = z_i\\)\n\n\nNote the following\n\nthe acceptance probability is \\(\\alpha = \\min(1, \\bar\\alpha)\\), but we do not need to calculate this explicitly as we can just compare \\(u\\) to \\(\\bar\\alpha\\)\nin step 2.4 we may reject a sample, but this does not mean that the chain is stuck at generating the same image - this is because the decoder is in fact sampling from \\(p_\\theta(x|z) = \\mathcal{N}(\\text{Decode}(x), I)\\). So the decoder will generate a different image for the same latent space.\n\nFor pre-conditioned Crank-Nicolson (pCN) sampling, the proposal, \\(\\mathcal{P}(z_i \\to z')\\), is given by\n\\[\nz' = \\sqrt{1 - \\beta^2} \\ z_i + \\beta \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I).\n\\]\nFor pCN, the acceptance threshold simplifies as follows:\n\\[\n\\bar\\alpha\n= \\frac{p(z'|\\mathcal{D}) \\mathcal{P}(z_i \\to z')}{p(z_i|\\mathcal{D}) \\mathcal{P}(z' \\to z_i)}  \n= \\frac{p(\\mathcal{D}|z')}{p(\\mathcal{D}|z_i)}\n\\]\nThis simplification works out due to the construction of the pCN proposal, which we prove later. We note that this simplification is a remarkable result, as it means that we only require the likelihood, \\(p(\\mathcal{D}|z)\\), to calculate the acceptance threshold, and do not deal with the prior or proposal distributions, which may otherwise result in a low acceptance rate.\nUnder our VAE model, the likelihood is given by our decoder:\n\\[\n\\begin{align*}\n  p(\\mathcal{D}|z)\n  &= \\prod_i p_\\theta(x^i|z)  \\\\\n  &\\propto \\exp\\left(-\\frac{1}{2} \\sum_i \\left\\|x^i - \\text{Decode}(z)\\right\\|_2^2\\right)\n  .\n\\end{align*}\n\\]\n\nExercise: Prove the pCN acceptance threshold simplification. (We ignore the measure theoretic details).",
    "crumbs": [
      "VAE"
    ]
  },
  {
    "objectID": "notes/VAE.html#experiment-ideas",
    "href": "notes/VAE.html#experiment-ideas",
    "title": "VAEs",
    "section": "",
    "text": "Visualise training of latent space in 2D / 3D - does this have the same representation as the classification exercise?\n\nThere are three phases to the KL loss (which is very apparent for latent_dim=8). Does this reflect in the latent space?!:\n\nKLD starts high, due to random initialisation of the model.\nKLD drops almost to 0, as the model learns to encode as a Gaussian very easily\nKLD rises sharply to compensate for the high MSE reconstruction loss.\nKLD increases slowly to match the MSE loss.\n\nUse a CNN / UNet as the encoder / decoder\n\n\n\nSo far, we have been able to visualise the latent space of a VAE for 2D and 3D latent spaces. We require a method to do this for higher dimensional latent spaces. Here are some options\n\nPotential methods to visulaise high dim space\n\nFor each dim, use violin / box plot to show distribution\n\nIf comparing two different runs, then use a violin plot with a violin for each run\n\nPerhaps can wrap these around a polar plot\n\nWe can plot e.g. the volume and direction of the latents. For higher dims, we could graph the volumne and directions\nFind path that traverses all digits in latent space\n\n\n\n\nWe would also want to visualise the effect of beta more clearly\n\nPerhaps use plotly with a slider?\nIs there a method to determine the optimal beta?\nshow how sliders affect the reconstruction: vary the value of each latent from -3 to 3 (3 sigmas)",
    "crumbs": [
      "VAE"
    ]
  },
  {
    "objectID": "notes/2-Backpropagation.html",
    "href": "notes/2-Backpropagation.html",
    "title": "How the backpropagation algorithm works",
    "section": "",
    "text": "This markdown file contains my notes on Chapter 2 of the book Neural Networks and Deep Learning. It is essentially a bank of equation derivations: - Mean Square Error Cost function - 4 Fundamental Equations - Error due to output layer weights - Recursive equation for layer error - Error due to bias - Error due to weight in any layer - Backpropagation algorithm\nThe backpropagation algorithm was introduced in the 70s, but was better understood in a famous 1986 paper by Rumelhart, Hinton, Williams\n\n\n\n\\(w_{jk}^l\\) refers to the \\(k\\)-th neuron in the \\(l-1\\)-th layer to the \\(j\\)-th neuron on the \\(l\\)-th layer.\n\\(a_j^l\\) is the activation of the \\(j\\)-th neuron in the \\(l\\)-th layer.\n\\(b_j^l\\) is the bias in the \\(j\\)-th neuron in the \\(l\\)-th layer\n\n\\[\na_j^l = \\sigma \\left( \\sum_k w_{jk}^l a_k^{l-1} + b_j^l \\right)\n\\]\nIn matrix form, we have:\n\\[\n\\begin{align*}\n  z^l &:= w^l \\cdot a^{l-1} + b^l \\\\\n  a &= \\sigma \\left ( z^l \\right)\n\\end{align*}\n\\]\n\n\\(z^l\\) is the weighted input to the neurons in the layer \\(l\\)\n\\(w^l\\) as the weight matrix defining the weights connecting the layer \\(l-1\\) to \\(l\\). i.e. the \\(w_{jk}^l\\) refers to the \\(j\\)-th row and \\(k\\)-th column.\n\n\n\n\n\nThe cost function \\(C_x\\) wrt to a training example \\(x\\) is independent of all other training examples: e.g. \\(C = \\frac{1}{n} \\sum_x C_x\\)\n\nThis is necessary so that \\(\\partial C/ \\partial w\\) can be calculated for each training example and aggregated in e.g. SGD\n\nThe cost function \\(C\\) can be written as a continuous function of the NN output (i.e the activations in last layer). e.g. \\(C(a^L) = \\lVert y - a^L \\rVert^2\\)\n\nNote: \\(y\\) is not a variable in the cost function \\(C\\) in the sense that \\(y\\) is a fixed parameter used to define the function.\n\n\n\n\n\nElement-wise product of two matrices of the same dimension commonly referred to as Hadamard product or Schur product:\n\\[\n\\begin{bmatrix}\n  1 \\\\ 2\n\\end{bmatrix}\n\\odot\n\\begin{bmatrix}\n  3 \\\\ 4\n\\end{bmatrix}\n  =\n\\begin{bmatrix}\n  1 \\times 3 \\\\ 2 \\times 4\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  3 \\\\ 8  \n\\end{bmatrix}\n\\]\n\n\n\nThe error in the cost wrt the weighted input in the \\(j\\)-th neuron in the \\(l\\)-th layer can be interpreted as \\(\\delta_j^L := \\frac{\\partial C}{\\partial z_j^l}\\). Intuitively, if the error is small, then we have reached a local minima.\nNote: For this section, we will use denominator-layout notation, which is the convention used by the book. However libraries such as pytorch and numpy use numerator-layout convention:\ni.e.\n\n\\(z\\) is a column vector\n\\(\\frac{\\partial C}{\\partial z}\\) is a column vector with elements \\(\\frac{\\partial C}{\\partial z_i}\\)\n\\(\\frac{\\partial a}{\\partial z}\\) is a matrix with the elements \\(J_{ij} = \\frac{\\partial a_j}{\\partial z_i}\\)\n\n\n\n\\[\n\\begin{align*}\n  \\vec{\\delta}^L &= \\frac{\\partial C(\\vec{a}^L)}{\\partial \\vec{z}^L} \\\\\n   &= \\left[\\frac{\\partial \\vec{a}^L}{\\partial \\vec{z}^L}\\right] \\frac{\\partial C(\\vec{a}^L)}{\\partial \\vec{a}^L}  \\\\\n  &= \\Sigma'(z^L) \\nabla_{a^L} C \\\\\n\\end{align*}\n\\]\nwhere \\(\\Sigma'\\) is a diagonal matrix because \\(\\frac{\\partial a_j^L}{\\partial z_i^L} = 0\\) for \\(i \\ne j\\) (i.e. \\(a_i^L\\) is does not depend on \\(z_j^L\\) for \\(i \\ne j\\)). Subbing in for our equations, this can be written in vectorised form:\n\\[\n\\begin{align*}\n   \\vec{\\delta}^L &= \\left[ \\frac{\\partial}{\\partial \\vec{a}^L} \\frac{1}{2} \\lVert \\vec{y} - \\vec{a}^L\\rVert_2^2 \\right] \\text{diag} \\left[\\frac{\\partial}{\\partial \\vec{z}^L} \\sigma(\\vec{z}_L)\\right] \\\\\n  &= (a^L - y) \\odot \\sigma'(z^L) \\\\\n\\end{align*}\n\\]\n\n\n\nEquation for the error \\(\\delta^l\\) in terms of the error in the next layer \\(\\delta^{l+1}\\)\n\\[\n\\begin{align*}\n  \\vec{\\delta}^l &= \\frac{\\partial C}{\\partial \\vec{z}^l} \\\\\n  &= \\left[ \\frac{\\partial z^{l+1}}{\\partial z^l} \\right] \\frac{\\partial C}{\\partial z^{l+1}}  \\\\\n  &= \\left[ \\frac{\\partial z^{l+1}}{\\partial z^l} \\right] \\delta^{l+1} \\\\\n  \\text{Consider:} \\\\\n  \\frac{\\partial}{\\partial z^l} \\{ z^{l+1} \\} &= \\frac{\\partial}{\\partial z^l} \\{ w^{l+1} \\sigma(z^l) + b^{l+1} \\} \\\\\n  \\implies \\frac{\\partial z^{l+1}}{\\partial z^l} &= \\left[ \\frac{\\partial \\vec{\\sigma}}{\\partial \\vec{z^l}} \\right] \\frac{\\partial}{\\partial \\vec{\\sigma}} w^{l+1} \\vec{\\sigma}  \\\\\n  &= \\Sigma'(z^l) \\ (w^{l+1})^T \\\\\n  \\text{Subbing in:} \\\\\n  \\delta^l &= \\Sigma'(z^l) \\ (w^{l+1})^T \\ \\delta^{l+1} \\\\\n  &= \\sigma'(z^l) \\odot (w^{l+1})^T \\delta^{l+1}\n\\end{align*}\n\\]\n\n\n\n\\[\n\\begin{align*}\n  \\frac{\\partial C}{\\partial \\vec{b}^l}\n  &= \\frac{\\partial \\vec{z}^l}{\\partial \\vec{b}^l} \\frac{\\partial C}{\\partial \\vec{z}^l} \\\\\n  &= \\frac{\\partial}{\\partial \\vec{b}^l} \\{w^la^{l-1} + b^l\\} \\vec{\\delta}^l \\\\\n  &= \\vec{\\delta}^l\n\\end{align*}\n\\]\n\n\n\nThis is better approached using index notation (because \\(\\partial z / \\partial w\\) is a 3rd order tensor!)\n\\[\n\\begin{align*}\n  \\frac{\\partial C}{\\partial w_{jk}^l}\n  &= \\frac{\\partial \\vec{z}^l}{\\partial w_{jk}^l} \\frac{\\partial C}{\\partial \\vec{z}^l} \\\\\n  &= \\frac{\\partial}{\\partial w_{jk}^l} \\{w^la^{l-1} + b^l\\} \\vec{\\delta}^l \\\\\n  &= a_k^{l-1} \\vec{\\delta}_j^l \\\\\n\\end{align*}\n\\]\nThus:\n\\[\n\\begin{align*}\n\\implies \\frac{\\partial C}{\\partial w^l} &= \\delta^l {a^{l-1}}^T \\\\\n&= a_\\text{in} \\delta_\\text{out}\n\\end{align*}\n\\]\nSome insights;\n\n\\(a_\\text{in} \\approx 0 \\implies \\partial C / \\partial w\\) will be small, hence the weights will learn slowly. i.e. low activation neurons learn slowly\nWhen \\(a \\approx \\in \\{0,1\\} \\implies \\sigma' \\approx 0\\), hence the weights and biases in the final layer will learn slowly if the output neuron is saturated.\nSimilarily if \\(\\sigma'(z^l) \\ll (w^{l+1})^T \\delta^{l+1}\\) , then the weights and biases in previous layers will learn slowly.\nTo get around saturation, one can use an activation function that has a constant / increasing gradient.\n\n\n\n\n\n\nInput \\(x\\) Set the corresponding activation for the input layer \\(a^{l=1}\\) (I believe the input literally just sets the activations in the first layer)\nFeedforward: For each \\(l=\\{1,2,\\ldots,L \\}\\), compute: \\(z^l\\) and \\(a^l\\)\nOutput error: Compute the output layer error \\(\\delta^L\\) (using BP1)\nBackpropagate the error: For each \\(l=\\{L-1, L=2, \\ldots, 2\\}\\) compute \\(\\delta^l\\) (using BP2)\nUpdate: Calculate the gradients wrt the weights and biases using BP3 and BP4 and update weights and biases. Note that when using SGD, the updates will be done on the average gradient of the mini-batch.\n\n\n\n\nBackpropagation with a single modified neuron\n\nIf the activation function of a single neuron was modified to \\(f(z) \\ne \\sigma(z)\\), then \\(\\sigma'(z^l)\\) would be modified such that the corresponding element would be \\(f'(z^l_k)\\)\n\nBackpropagation with linear neurons\n\nThis corresponds to setting \\(\\sigma'(z^l) = \\vec{1}\\)\n\n\n\n\nBackpropagation requires two passes through the network: forward and backward, which are both similar in complexity (Note that there are no inverse matrices required to be calculated!).\nA naive approach may be to find \\(\\partial C / \\partial w_{jk}^l\\) for each weight by using a first order apporximation and running the network twice to find \\(C(w), C(w + \\epsilon)\\). This scales with the number of parameters, which is incredibly slow. Conversely, backpropagation utilises the chain rule to update all the parameters based of one observation, by utilising the relationship between the weights.",
    "crumbs": [
      "Notes",
      "Backpropagation"
    ]
  },
  {
    "objectID": "notes/2-Backpropagation.html#a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network",
    "href": "notes/2-Backpropagation.html#a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network",
    "title": "How the backpropagation algorithm works",
    "section": "",
    "text": "\\(w_{jk}^l\\) refers to the \\(k\\)-th neuron in the \\(l-1\\)-th layer to the \\(j\\)-th neuron on the \\(l\\)-th layer.\n\\(a_j^l\\) is the activation of the \\(j\\)-th neuron in the \\(l\\)-th layer.\n\\(b_j^l\\) is the bias in the \\(j\\)-th neuron in the \\(l\\)-th layer\n\n\\[\na_j^l = \\sigma \\left( \\sum_k w_{jk}^l a_k^{l-1} + b_j^l \\right)\n\\]\nIn matrix form, we have:\n\\[\n\\begin{align*}\n  z^l &:= w^l \\cdot a^{l-1} + b^l \\\\\n  a &= \\sigma \\left ( z^l \\right)\n\\end{align*}\n\\]\n\n\\(z^l\\) is the weighted input to the neurons in the layer \\(l\\)\n\\(w^l\\) as the weight matrix defining the weights connecting the layer \\(l-1\\) to \\(l\\). i.e. the \\(w_{jk}^l\\) refers to the \\(j\\)-th row and \\(k\\)-th column.",
    "crumbs": [
      "Notes",
      "Backpropagation"
    ]
  },
  {
    "objectID": "notes/2-Backpropagation.html#assumption-we-need-about-the-cost-function",
    "href": "notes/2-Backpropagation.html#assumption-we-need-about-the-cost-function",
    "title": "How the backpropagation algorithm works",
    "section": "",
    "text": "The cost function \\(C_x\\) wrt to a training example \\(x\\) is independent of all other training examples: e.g. \\(C = \\frac{1}{n} \\sum_x C_x\\)\n\nThis is necessary so that \\(\\partial C/ \\partial w\\) can be calculated for each training example and aggregated in e.g. SGD\n\nThe cost function \\(C\\) can be written as a continuous function of the NN output (i.e the activations in last layer). e.g. \\(C(a^L) = \\lVert y - a^L \\rVert^2\\)\n\nNote: \\(y\\) is not a variable in the cost function \\(C\\) in the sense that \\(y\\) is a fixed parameter used to define the function.",
    "crumbs": [
      "Notes",
      "Backpropagation"
    ]
  },
  {
    "objectID": "notes/2-Backpropagation.html#hadamard-product",
    "href": "notes/2-Backpropagation.html#hadamard-product",
    "title": "How the backpropagation algorithm works",
    "section": "",
    "text": "Element-wise product of two matrices of the same dimension commonly referred to as Hadamard product or Schur product:\n\\[\n\\begin{bmatrix}\n  1 \\\\ 2\n\\end{bmatrix}\n\\odot\n\\begin{bmatrix}\n  3 \\\\ 4\n\\end{bmatrix}\n  =\n\\begin{bmatrix}\n  1 \\times 3 \\\\ 2 \\times 4\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  3 \\\\ 8  \n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Notes",
      "Backpropagation"
    ]
  },
  {
    "objectID": "notes/2-Backpropagation.html#fundamental-equations-behind-backpropagation",
    "href": "notes/2-Backpropagation.html#fundamental-equations-behind-backpropagation",
    "title": "How the backpropagation algorithm works",
    "section": "",
    "text": "The error in the cost wrt the weighted input in the \\(j\\)-th neuron in the \\(l\\)-th layer can be interpreted as \\(\\delta_j^L := \\frac{\\partial C}{\\partial z_j^l}\\). Intuitively, if the error is small, then we have reached a local minima.\nNote: For this section, we will use denominator-layout notation, which is the convention used by the book. However libraries such as pytorch and numpy use numerator-layout convention:\ni.e.\n\n\\(z\\) is a column vector\n\\(\\frac{\\partial C}{\\partial z}\\) is a column vector with elements \\(\\frac{\\partial C}{\\partial z_i}\\)\n\\(\\frac{\\partial a}{\\partial z}\\) is a matrix with the elements \\(J_{ij} = \\frac{\\partial a_j}{\\partial z_i}\\)\n\n\n\n\\[\n\\begin{align*}\n  \\vec{\\delta}^L &= \\frac{\\partial C(\\vec{a}^L)}{\\partial \\vec{z}^L} \\\\\n   &= \\left[\\frac{\\partial \\vec{a}^L}{\\partial \\vec{z}^L}\\right] \\frac{\\partial C(\\vec{a}^L)}{\\partial \\vec{a}^L}  \\\\\n  &= \\Sigma'(z^L) \\nabla_{a^L} C \\\\\n\\end{align*}\n\\]\nwhere \\(\\Sigma'\\) is a diagonal matrix because \\(\\frac{\\partial a_j^L}{\\partial z_i^L} = 0\\) for \\(i \\ne j\\) (i.e. \\(a_i^L\\) is does not depend on \\(z_j^L\\) for \\(i \\ne j\\)). Subbing in for our equations, this can be written in vectorised form:\n\\[\n\\begin{align*}\n   \\vec{\\delta}^L &= \\left[ \\frac{\\partial}{\\partial \\vec{a}^L} \\frac{1}{2} \\lVert \\vec{y} - \\vec{a}^L\\rVert_2^2 \\right] \\text{diag} \\left[\\frac{\\partial}{\\partial \\vec{z}^L} \\sigma(\\vec{z}_L)\\right] \\\\\n  &= (a^L - y) \\odot \\sigma'(z^L) \\\\\n\\end{align*}\n\\]\n\n\n\nEquation for the error \\(\\delta^l\\) in terms of the error in the next layer \\(\\delta^{l+1}\\)\n\\[\n\\begin{align*}\n  \\vec{\\delta}^l &= \\frac{\\partial C}{\\partial \\vec{z}^l} \\\\\n  &= \\left[ \\frac{\\partial z^{l+1}}{\\partial z^l} \\right] \\frac{\\partial C}{\\partial z^{l+1}}  \\\\\n  &= \\left[ \\frac{\\partial z^{l+1}}{\\partial z^l} \\right] \\delta^{l+1} \\\\\n  \\text{Consider:} \\\\\n  \\frac{\\partial}{\\partial z^l} \\{ z^{l+1} \\} &= \\frac{\\partial}{\\partial z^l} \\{ w^{l+1} \\sigma(z^l) + b^{l+1} \\} \\\\\n  \\implies \\frac{\\partial z^{l+1}}{\\partial z^l} &= \\left[ \\frac{\\partial \\vec{\\sigma}}{\\partial \\vec{z^l}} \\right] \\frac{\\partial}{\\partial \\vec{\\sigma}} w^{l+1} \\vec{\\sigma}  \\\\\n  &= \\Sigma'(z^l) \\ (w^{l+1})^T \\\\\n  \\text{Subbing in:} \\\\\n  \\delta^l &= \\Sigma'(z^l) \\ (w^{l+1})^T \\ \\delta^{l+1} \\\\\n  &= \\sigma'(z^l) \\odot (w^{l+1})^T \\delta^{l+1}\n\\end{align*}\n\\]\n\n\n\n\\[\n\\begin{align*}\n  \\frac{\\partial C}{\\partial \\vec{b}^l}\n  &= \\frac{\\partial \\vec{z}^l}{\\partial \\vec{b}^l} \\frac{\\partial C}{\\partial \\vec{z}^l} \\\\\n  &= \\frac{\\partial}{\\partial \\vec{b}^l} \\{w^la^{l-1} + b^l\\} \\vec{\\delta}^l \\\\\n  &= \\vec{\\delta}^l\n\\end{align*}\n\\]\n\n\n\nThis is better approached using index notation (because \\(\\partial z / \\partial w\\) is a 3rd order tensor!)\n\\[\n\\begin{align*}\n  \\frac{\\partial C}{\\partial w_{jk}^l}\n  &= \\frac{\\partial \\vec{z}^l}{\\partial w_{jk}^l} \\frac{\\partial C}{\\partial \\vec{z}^l} \\\\\n  &= \\frac{\\partial}{\\partial w_{jk}^l} \\{w^la^{l-1} + b^l\\} \\vec{\\delta}^l \\\\\n  &= a_k^{l-1} \\vec{\\delta}_j^l \\\\\n\\end{align*}\n\\]\nThus:\n\\[\n\\begin{align*}\n\\implies \\frac{\\partial C}{\\partial w^l} &= \\delta^l {a^{l-1}}^T \\\\\n&= a_\\text{in} \\delta_\\text{out}\n\\end{align*}\n\\]\nSome insights;\n\n\\(a_\\text{in} \\approx 0 \\implies \\partial C / \\partial w\\) will be small, hence the weights will learn slowly. i.e. low activation neurons learn slowly\nWhen \\(a \\approx \\in \\{0,1\\} \\implies \\sigma' \\approx 0\\), hence the weights and biases in the final layer will learn slowly if the output neuron is saturated.\nSimilarily if \\(\\sigma'(z^l) \\ll (w^{l+1})^T \\delta^{l+1}\\) , then the weights and biases in previous layers will learn slowly.\nTo get around saturation, one can use an activation function that has a constant / increasing gradient.",
    "crumbs": [
      "Notes",
      "Backpropagation"
    ]
  },
  {
    "objectID": "notes/2-Backpropagation.html#backpropagation-algorithm",
    "href": "notes/2-Backpropagation.html#backpropagation-algorithm",
    "title": "How the backpropagation algorithm works",
    "section": "",
    "text": "Input \\(x\\) Set the corresponding activation for the input layer \\(a^{l=1}\\) (I believe the input literally just sets the activations in the first layer)\nFeedforward: For each \\(l=\\{1,2,\\ldots,L \\}\\), compute: \\(z^l\\) and \\(a^l\\)\nOutput error: Compute the output layer error \\(\\delta^L\\) (using BP1)\nBackpropagate the error: For each \\(l=\\{L-1, L=2, \\ldots, 2\\}\\) compute \\(\\delta^l\\) (using BP2)\nUpdate: Calculate the gradients wrt the weights and biases using BP3 and BP4 and update weights and biases. Note that when using SGD, the updates will be done on the average gradient of the mini-batch.\n\n\n\n\nBackpropagation with a single modified neuron\n\nIf the activation function of a single neuron was modified to \\(f(z) \\ne \\sigma(z)\\), then \\(\\sigma'(z^l)\\) would be modified such that the corresponding element would be \\(f'(z^l_k)\\)\n\nBackpropagation with linear neurons\n\nThis corresponds to setting \\(\\sigma'(z^l) = \\vec{1}\\)",
    "crumbs": [
      "Notes",
      "Backpropagation"
    ]
  },
  {
    "objectID": "notes/2-Backpropagation.html#in-what-sense-is-backpropagation-fast",
    "href": "notes/2-Backpropagation.html#in-what-sense-is-backpropagation-fast",
    "title": "How the backpropagation algorithm works",
    "section": "",
    "text": "Backpropagation requires two passes through the network: forward and backward, which are both similar in complexity (Note that there are no inverse matrices required to be calculated!).\nA naive approach may be to find \\(\\partial C / \\partial w_{jk}^l\\) for each weight by using a first order apporximation and running the network twice to find \\(C(w), C(w + \\epsilon)\\). This scales with the number of parameters, which is incredibly slow. Conversely, backpropagation utilises the chain rule to update all the parameters based of one observation, by utilising the relationship between the weights.",
    "crumbs": [
      "Notes",
      "Backpropagation"
    ]
  },
  {
    "objectID": "notes/1-NNs_Intro.html",
    "href": "notes/1-NNs_Intro.html",
    "title": "Chapter 1: Using neural nets to recognize handwritten digits",
    "section": "",
    "text": "This markdown file contains my notes on Chapter 1 of the book Neural Networks and Deep Learning. - Perceptron / Sigmoid-Neurons - Architecture - Stochastic Gradient Descent - No hidden layer network (= linear model!)\n\n\n\\[\n\\text{output} =\n  \\begin{cases}\n    0 \\ \\text{if} \\ w \\cdot x + b \\le 0 \\\\\n    1 \\ \\text{if} \\ w \\cdot x + b &gt; 0\n  \\end{cases}\n\\]\n\nbias is a measure of heasy it is to get the perceptron to output a 1\n\nA network of perceptrons can be equivalent to a NAND gate:\n\n\n\nNAND gate equivalent perceptrons\n\n\n\nThis means that perceptrons are “no better” than our basis of computation.\nHow do we implement “learning”!\n\n\n\n\nAim: Make neuron differntiable\n\\[\ny = \\sigma(w \\cdot x + b)\n\\]\nSigmoid function is used because it has good analytical properties when differentiating.\n\n\n\nAlt text\n\n\nNeuron becomes a perceptron if the activation function is a step: \\(y = H(w \\cdot x + b)\\) (actually \\(H(0) = 0\\), but a perceptron is 1).\n\n\n\nAlt text\n\n\n\n\n\n\n\nSigmoid neurons simulating perceptrons, part I\n\n\nMultiplying by a constant \\(c &gt; 0\\) will not change the network, because \\(H(cz) = H(c)\\). Also, considering the decision boundary algebraically\n\\[\n\\begin{align*}\n  \\text{Decision boundary} \\\\\n  w \\cdot x + b &\\le 0 \\\\\n  \\text{Multiplying by } c \\\\\n  c \\ (w \\cdot x + b) &\\le 0 \\\\\n  \\implies w \\cdot x + b &\\le 0 \\\\\n\\end{align*}\n\\]\n\n\nSigmoid neurons simulating perceptrons, part II\n\n\nIn the limit \\(c \\to \\inf \\implies \\sigma(cz) \\to H(z)\\), hence a sigmoid neuron tends to a perceptron. Note that this becomes unstable / fails when \\(z = 0\\), because it is unclear which size the sigmoid will predict.\n\n\n\n\n\n\nNote: Confusingly, these use sigmoid activation functions (as opposed to perceptrons)\n\n\n\n\nAlt text\n\n\nThere are some design / engineering heuristics used when deciding on the hidden layers:\n\nTODO\n\nOther networks:\n\nFeed forward networks\n\ninformation flows forwards\nMLP is a type of FFN\n\nRecurrent Neural Networks\n\ninformation can flow in loops\nSome neurons fire for some time before becoming quiscient\n\n\n\n\n\n\n\n\nAlt text\n\n\nIntersting thought: The output layer is composed of 10 Neurons. What if this was changed to a bitwise representation composed of 4 neurons?\nAns: In some sense, the neurons fire based on part of the shape of a digit. It would be difficult to associate that to the most significant bit of a digit compared to the number itself!\n\nThere is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts decimal to binary:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n\\(B_3\\)\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\\(B_2\\)\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n\n\n\\(B_1\\)\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n\n\n\\(B_0\\)\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n\nFrom this table, it is obvious that the weights of the neurons are simply \\(w_\\text{binary, decimal}\\) as per the table above. (Assuming that the correct output has activation &gt; 0.99, and the incorrect activation &lt; 0.01).\n\n\n\n\nCost function / loss function:\n\\[\nC(w,b) := \\frac{1}{2n} \\sum_x \\lVert y(x) - a \\rVert_2 ^2\n\\]\nwhere, \\(a\\) is the output from the network.\n\n\\(C\\) is commonly referred to as quadratic cost or mean square error (MSE) cost.\nNote: Quadratic cost is a useful proxy as a measure of accuracy. This cost function is useful because it’s differentiable, whereas the accuracy (precision) is not!\n\nConsider the gradient of this:\n\\[\n\\Delta C \\approx \\nabla C \\cdot \\Delta v\n\\]\nwhere \\(\\Delta v\\) is the change in the inputs to the Cost function. Consider choosing \\(\\Delta v\\):\n\\[\n\\begin{align*}\n  \\Delta v &= - \\eta \\nabla C \\\\\n  \\implies \\Delta C &= \\nabla C \\cdot - \\eta \\nabla C \\\\\n  &= - \\eta \\lVert \\nabla C \\rVert^2 \\\\\n  & \\le 0\n\\end{align*}\n\\]\nThus if we could choose how to change the input parameters, then we could traverse the cost function so that it is always decreasing until reaching a local minimum.\n\\[\nv \\to v' = v - \\eta \\nabla C\n\\]\n\nThe equation only applies locally, so the learning rate \\(\\eta\\) needs to be chosen small enough such that \\(\\Delta C \\le 0\\) holds.\nOn the flip side, we don’t want the learning rate to be too small, otherwise this algorithm will converge too slowly, or will be stuck in a local minimum.\nIn practice, the learning rate is varied\n\n\n\n\nGiven a fixed step size \\(\\lVert \\Delta v \\rVert = \\epsilon &gt; 0\\), to minimise \\(\\Delta C = \\nabla C \\cdot \\Delta v\\), the optimal move is \\(\\Delta v = - \\eta \\nabla C\\), where \\(\\eta = \\epsilon / \\lVert \\nabla C \\rVert\\).\n\nTo prove this, consider \\(\\Delta v = - \\eta \\nabla C + u\\) for some \\(u\\) such that \\(\\Delta v\\) represents any vector.\n\\[\n\\begin{align*}\n\\lVert \\Delta C \\rVert & \\approx \\lVert \\nabla C \\cdot \\Delta v \\rVert \\\\\n&= \\lVert \\nabla C \\cdot (- \\eta \\nabla C + u) \\rVert \\\\\n&= \\lVert - \\eta \\nabla C \\cdot \\nabla C + \\nabla C \\cdot u \\rVert \\\\\n\\text{Cauchy-Schwarz inequality:}\\\\\n& \\le \\lVert - \\eta \\nabla C \\cdot \\nabla C \\rVert + \\lVert \\nabla C \\cdot u \\rVert \\\\\n&= \\eta \\lVert \\nabla C \\rVert^2 + \\lVert \\nabla C \\cdot u \\rVert \\\\\n\\text{Sub in for } \\eta \\\\\n&= \\epsilon \\lVert \\nabla C \\rVert (1 + u) \\\\\n\\end{align*}\n\\]\nThis expression is obviously minimised for \\(u = 0\\) as required.\n\nWhat happens when \\(C\\) is a function of one variable?\n\nWhen \\(C\\) is multivariable, \\(\\Delta C\\) is the directional derivative in the direction of \\(\\Delta v\\). Note that \\(\\nabla C\\) points in the direction of maximum increase, hence by choosing \\(\\Delta v \\propto - \\nabla C\\), we traverse \\(C\\) in the direction of maximum of descent. In 1D, this corresponds to moving left/right.\n\n\n\nIt is computationally intensive to calculate the cost function for all training inputs \\(x\\). Instead, a mini-batch of size \\(m\\) is used to estimate the gradient:\n\\[\n\\Delta C = \\frac{1}{n} \\sum_x C_x \\approx \\frac{1}{m} \\sum_{j=1}^m  C_{x_j}\n\\]\nGradients are recalculated for each mini-batch until all training data is used up. This is equivalent to one epoch.\nInterestingly, if the approximation is good enough, then we perform the descent a lot quicker, because after one epoch, we would have performed \\(n/m\\) updates instead of just 1!\n\n\n\n\nAn extreme version of gradient descent is to use a mini-batch size of just 1… This procedure is known as online, on-line, or incremental learning.\n\nAdv: Updates very fast (every time after seeing a new training example) Dadv: Can overfit to the training example\n\n\n\n\n\nSee chap1.ipynb to see use of pytorch to train a Neural Network.\nInterestingly, VSMs can get a performance close to 98.5% accuracy.\n\n\n\n\nTry creating a network with just two layers\n\nThis achieved an accuracy of 91% with learning rate = 1, mini batch = 30. This is a lot better than I expected! Though, with a hidden layer of 30 and learning rate=3, a much better performance of 95% was achieved see this post and the sklearn tutorial on RBFs with SVM.\nInterestingly, having no hidden layer is equivalent to learning from a linear model.\n\\[\n\\begin{align*}\n\\text{Neurons for Digit 1 } y_1: \\\\\nz_1 &= w_1 \\cdot x + b\\\\\ny_1  &= \\sigma(w_1 \\cdot x + b) \\\\\n\\text{Gradient} \\\\\n\\frac{\\partial y_1}{\\partial x} &= \\frac{\\partial \\sigma}{\\partial z_1} \\frac{\\partial z_1}{\\partial x}\n\\\\\n\\sigma \\text{ is monotonic, so} \\\\\n\\left[\\frac{\\partial y_1}{\\partial x}\\right]\n&= \\left[ \\frac{\\partial z_1}{\\partial x} \\right]\n= \\vec{0}\n\\\\\n\\text{at a stationary point.}\\\\ \\\\\n\\end{align*}\n\\]\nNote this is a heuristic calculation (we should really be comparing the gradients of the cost function, but the idea is the same). Since \\(\\sigma\\) is monotonic, the theorerical global optimum will be the same as a linear model! However, since we use SGD instead of say OLS (ordinary least squares), we may trade-off being less overfit at the expsnse of hitting a local optimum (i.e performing worse on the training set but perhaps better on the validation set).\nNote also:\n\\[\n\\text{sign}\\left[\\frac{\\partial y_1}{\\partial x}\\right]\n= \\text{sign} \\left[ \\frac{\\partial z_1}{\\partial x} \\right]\n\\]\nThis is the elementwise \\(\\text{sign}\\) function: essentially each element will have the same sign, hence the gradients will be pointing in the same direction. This means that the gradient updates will be in the same direction as if it were a linear model (though the magnitude of the updates will be differen due to the non-linear sigmoid scaling).\nFun:\n\\[\n\\text{sophisticated algorithm} \\le \\text{simple learning algorithm} + \\text{good training data}\n\\]\n\n\n\n\n\nNN are hardly intepretable\nNNs with many hidden layers didn’t perform very well in the 80s or 90s. Some techniques in 2006 were introduced which drastically improved this.",
    "crumbs": [
      "Notes",
      "Intro to Neural Networks"
    ]
  },
  {
    "objectID": "notes/1-NNs_Intro.html#architecture-of-nns",
    "href": "notes/1-NNs_Intro.html#architecture-of-nns",
    "title": "Chapter 1: Using neural nets to recognize handwritten digits",
    "section": "",
    "text": "Note: Confusingly, these use sigmoid activation functions (as opposed to perceptrons)\n\n\n\n\nAlt text\n\n\nThere are some design / engineering heuristics used when deciding on the hidden layers:\n\nTODO\n\nOther networks:\n\nFeed forward networks\n\ninformation flows forwards\nMLP is a type of FFN\n\nRecurrent Neural Networks\n\ninformation can flow in loops\nSome neurons fire for some time before becoming quiscient\n\n\n\n\n\n\n\n\nAlt text\n\n\nIntersting thought: The output layer is composed of 10 Neurons. What if this was changed to a bitwise representation composed of 4 neurons?\nAns: In some sense, the neurons fire based on part of the shape of a digit. It would be difficult to associate that to the most significant bit of a digit compared to the number itself!\n\nThere is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts decimal to binary:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n\\(B_3\\)\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\\(B_2\\)\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n\n\n\\(B_1\\)\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n\n\n\\(B_0\\)\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n\nFrom this table, it is obvious that the weights of the neurons are simply \\(w_\\text{binary, decimal}\\) as per the table above. (Assuming that the correct output has activation &gt; 0.99, and the incorrect activation &lt; 0.01).",
    "crumbs": [
      "Notes",
      "Intro to Neural Networks"
    ]
  },
  {
    "objectID": "notes/1-NNs_Intro.html#learning-with-gradient-descent",
    "href": "notes/1-NNs_Intro.html#learning-with-gradient-descent",
    "title": "Chapter 1: Using neural nets to recognize handwritten digits",
    "section": "",
    "text": "Cost function / loss function:\n\\[\nC(w,b) := \\frac{1}{2n} \\sum_x \\lVert y(x) - a \\rVert_2 ^2\n\\]\nwhere, \\(a\\) is the output from the network.\n\n\\(C\\) is commonly referred to as quadratic cost or mean square error (MSE) cost.\nNote: Quadratic cost is a useful proxy as a measure of accuracy. This cost function is useful because it’s differentiable, whereas the accuracy (precision) is not!\n\nConsider the gradient of this:\n\\[\n\\Delta C \\approx \\nabla C \\cdot \\Delta v\n\\]\nwhere \\(\\Delta v\\) is the change in the inputs to the Cost function. Consider choosing \\(\\Delta v\\):\n\\[\n\\begin{align*}\n  \\Delta v &= - \\eta \\nabla C \\\\\n  \\implies \\Delta C &= \\nabla C \\cdot - \\eta \\nabla C \\\\\n  &= - \\eta \\lVert \\nabla C \\rVert^2 \\\\\n  & \\le 0\n\\end{align*}\n\\]\nThus if we could choose how to change the input parameters, then we could traverse the cost function so that it is always decreasing until reaching a local minimum.\n\\[\nv \\to v' = v - \\eta \\nabla C\n\\]\n\nThe equation only applies locally, so the learning rate \\(\\eta\\) needs to be chosen small enough such that \\(\\Delta C \\le 0\\) holds.\nOn the flip side, we don’t want the learning rate to be too small, otherwise this algorithm will converge too slowly, or will be stuck in a local minimum.\nIn practice, the learning rate is varied\n\n\n\n\nGiven a fixed step size \\(\\lVert \\Delta v \\rVert = \\epsilon &gt; 0\\), to minimise \\(\\Delta C = \\nabla C \\cdot \\Delta v\\), the optimal move is \\(\\Delta v = - \\eta \\nabla C\\), where \\(\\eta = \\epsilon / \\lVert \\nabla C \\rVert\\).\n\nTo prove this, consider \\(\\Delta v = - \\eta \\nabla C + u\\) for some \\(u\\) such that \\(\\Delta v\\) represents any vector.\n\\[\n\\begin{align*}\n\\lVert \\Delta C \\rVert & \\approx \\lVert \\nabla C \\cdot \\Delta v \\rVert \\\\\n&= \\lVert \\nabla C \\cdot (- \\eta \\nabla C + u) \\rVert \\\\\n&= \\lVert - \\eta \\nabla C \\cdot \\nabla C + \\nabla C \\cdot u \\rVert \\\\\n\\text{Cauchy-Schwarz inequality:}\\\\\n& \\le \\lVert - \\eta \\nabla C \\cdot \\nabla C \\rVert + \\lVert \\nabla C \\cdot u \\rVert \\\\\n&= \\eta \\lVert \\nabla C \\rVert^2 + \\lVert \\nabla C \\cdot u \\rVert \\\\\n\\text{Sub in for } \\eta \\\\\n&= \\epsilon \\lVert \\nabla C \\rVert (1 + u) \\\\\n\\end{align*}\n\\]\nThis expression is obviously minimised for \\(u = 0\\) as required.\n\nWhat happens when \\(C\\) is a function of one variable?\n\nWhen \\(C\\) is multivariable, \\(\\Delta C\\) is the directional derivative in the direction of \\(\\Delta v\\). Note that \\(\\nabla C\\) points in the direction of maximum increase, hence by choosing \\(\\Delta v \\propto - \\nabla C\\), we traverse \\(C\\) in the direction of maximum of descent. In 1D, this corresponds to moving left/right.\n\n\n\nIt is computationally intensive to calculate the cost function for all training inputs \\(x\\). Instead, a mini-batch of size \\(m\\) is used to estimate the gradient:\n\\[\n\\Delta C = \\frac{1}{n} \\sum_x C_x \\approx \\frac{1}{m} \\sum_{j=1}^m  C_{x_j}\n\\]\nGradients are recalculated for each mini-batch until all training data is used up. This is equivalent to one epoch.\nInterestingly, if the approximation is good enough, then we perform the descent a lot quicker, because after one epoch, we would have performed \\(n/m\\) updates instead of just 1!\n\n\n\n\nAn extreme version of gradient descent is to use a mini-batch size of just 1… This procedure is known as online, on-line, or incremental learning.\n\nAdv: Updates very fast (every time after seeing a new training example) Dadv: Can overfit to the training example",
    "crumbs": [
      "Notes",
      "Intro to Neural Networks"
    ]
  },
  {
    "objectID": "notes/1-NNs_Intro.html#implementing-our-network-to-classify-digits",
    "href": "notes/1-NNs_Intro.html#implementing-our-network-to-classify-digits",
    "title": "Chapter 1: Using neural nets to recognize handwritten digits",
    "section": "",
    "text": "See chap1.ipynb to see use of pytorch to train a Neural Network.\nInterestingly, VSMs can get a performance close to 98.5% accuracy.\n\n\n\n\nTry creating a network with just two layers\n\nThis achieved an accuracy of 91% with learning rate = 1, mini batch = 30. This is a lot better than I expected! Though, with a hidden layer of 30 and learning rate=3, a much better performance of 95% was achieved see this post and the sklearn tutorial on RBFs with SVM.\nInterestingly, having no hidden layer is equivalent to learning from a linear model.\n\\[\n\\begin{align*}\n\\text{Neurons for Digit 1 } y_1: \\\\\nz_1 &= w_1 \\cdot x + b\\\\\ny_1  &= \\sigma(w_1 \\cdot x + b) \\\\\n\\text{Gradient} \\\\\n\\frac{\\partial y_1}{\\partial x} &= \\frac{\\partial \\sigma}{\\partial z_1} \\frac{\\partial z_1}{\\partial x}\n\\\\\n\\sigma \\text{ is monotonic, so} \\\\\n\\left[\\frac{\\partial y_1}{\\partial x}\\right]\n&= \\left[ \\frac{\\partial z_1}{\\partial x} \\right]\n= \\vec{0}\n\\\\\n\\text{at a stationary point.}\\\\ \\\\\n\\end{align*}\n\\]\nNote this is a heuristic calculation (we should really be comparing the gradients of the cost function, but the idea is the same). Since \\(\\sigma\\) is monotonic, the theorerical global optimum will be the same as a linear model! However, since we use SGD instead of say OLS (ordinary least squares), we may trade-off being less overfit at the expsnse of hitting a local optimum (i.e performing worse on the training set but perhaps better on the validation set).\nNote also:\n\\[\n\\text{sign}\\left[\\frac{\\partial y_1}{\\partial x}\\right]\n= \\text{sign} \\left[ \\frac{\\partial z_1}{\\partial x} \\right]\n\\]\nThis is the elementwise \\(\\text{sign}\\) function: essentially each element will have the same sign, hence the gradients will be pointing in the same direction. This means that the gradient updates will be in the same direction as if it were a linear model (though the magnitude of the updates will be differen due to the non-linear sigmoid scaling).\nFun:\n\\[\n\\text{sophisticated algorithm} \\le \\text{simple learning algorithm} + \\text{good training data}\n\\]",
    "crumbs": [
      "Notes",
      "Intro to Neural Networks"
    ]
  },
  {
    "objectID": "notes/1-NNs_Intro.html#toward-deep-learning",
    "href": "notes/1-NNs_Intro.html#toward-deep-learning",
    "title": "Chapter 1: Using neural nets to recognize handwritten digits",
    "section": "",
    "text": "NN are hardly intepretable\nNNs with many hidden layers didn’t perform very well in the 80s or 90s. Some techniques in 2006 were introduced which drastically improved this.",
    "crumbs": [
      "Notes",
      "Intro to Neural Networks"
    ]
  },
  {
    "objectID": "notes/3-Improving_Learning.html",
    "href": "notes/3-Improving_Learning.html",
    "title": "Improving the way neural networks learn",
    "section": "",
    "text": "This markdown file contains my notes on Chapter 3 of the book Neural Networks and Deep Learning. This chapter will introduce many techniques to improve the network learning including:\n\nCross Entropy cost function\nSoftmax on the output layer\n4 Regularisation methods\n\nL1 / L2 regularisation\ndropout\nartificial expansion of the training data\n\nWeight Initialisation\nHeuristics for tuning Hyperparameters\nVariations of Gradient Descent\nOther activation funcitons: Tanh, ReLu\n\n\n\nMotivation: As seen in Chap 2, a neuron learns very slowly if the activation function is saturated (i.e. \\(z\\) is at an extreme such that the gradient \\(\\sigma'(z)\\) is too small to cause any substantial change in the update step). Consider the weights in the final layer:\n\\[\n\\begin{align*}\n\\frac{\\partial C_{MSE}}{\\partial w^L} &= \\delta^L {a^{L-1}}^T \\\\\n&= (a^L - y) \\odot \\sigma'(z^L) {a^{L-1}}^T \\\\\n& \\propto \\sigma'(z^L)\n\\end{align*}\n\\]\nCross-Entropy loss resolves this by removing the dependency on the gradient of the sigmoid activation function. The CE loss function \\(C_{CE}\\) is given by:\n\\[\n\\begin{align*}\n  C_{CE} &= \\frac{1}{n} \\sum_x C_{CE,x} \\\\\n  \\text{where:}\\\\\n  C_{CE,x} &= - \\sum_j \\left[y_j \\ln a_j^L + (1 - y_j) \\ln(1 - a_j^L) \\right] \\\\\n  &= -y \\cdot \\ln (a^L) - (1 - y) \\cdot \\ln (1 - a^L)\n\\end{align*}\n\\]\nTo find the gradient wrt to a weight\n\\[\n\\begin{align*}\n\\frac{\\partial C_{CE,x}}{\\partial w^L}\n&= \\frac{\\partial z^L}{\\partial w^L} \\frac{\\partial a^L}{\\partial z^L} \\frac{\\partial C_{CE,x}}{\\partial a^L} \\\\\n\\text{Note } a^L = \\sigma (z^L) \\\\\n&= ({a^{L-1}}^T \\otimes I ) \\ \\sigma'(z^L) \\cdot \\left[ -y \\cdot \\frac{1}{\\sigma (z^L)} +(1 - y) \\cdot \\frac{1}{(1 - \\sigma(z^L))}\\right] \\\\\n&= ({a^{L-1}}^T \\otimes I ) \\ \\sigma'(z^L) \\cdot \\left[ \\frac{\\sigma(z^L) - y}{\\sigma(z^L)(1 - \\sigma(z^L))}\\right] \\\\\n\\text{Note } \\sigma' = \\sigma(1 - \\sigma) \\\\\n&= \\left({a^{L-1}}^T \\otimes I \\right) \\cdot \\left( \\sigma(z^L) - y \\right) \\\\\n&= \\left( \\sigma(z^L) - y \\right) {a^{L-1}}^T \\\\\n\\end{align*}\n\\]\ni.e. the gradient is proprtional to the error - this makes much more sense, because now the network will learn quicker if the error is larger (which is much more human like as well). (Note: vector-matrix derivative and Kronecher product, though the last line is dubious.)\n\n\n\nThe right form of cross-entropy\n\nConsider the incorrect form; \\(- [a \\ln y + (1 - a) \\ln (1 - y)]\\). In a classification problem, \\(y \\in \\{0,1\\}\\) and in either case, the cost will be very large and positive: \\(+\\inf \\times \\{a, 1-a\\}\\).\n\nRegression problems\n\nIn regression problems, \\(y \\in [0,1]\\). The cross entropy will still be minimised when \\(a(z) = y\\) by Gibb’s inequality.\n\nUsing the quadratic cost when we have linear neurons in the output layer\n\nThen we have:\n\\[\n\\begin{align*}\n\\frac{\\partial C_{MSE,x}}{\\partial w^L} &= \\delta^L {a^{L-1}}^T \\\\\n&= (a^L - y) {a^{L-1}}^T \\\\\n\\end{align*}\n\\]\nThis resolves the saturation issue in the final layer, however the issue still remains in the previous layers.\n\n\n\nWorking backwards, we can derive the CE loss by supposing that we want to satisfy this differential equation for the bias:\n\\[\n\\begin{align*}\n  \\frac{\\partial C}{\\partial b} &= (a - y) \\\\\n  \\text{Using the chain rule} \\\\\n  &= \\frac{\\partial z}{\\partial b} \\frac{\\partial a}{\\partial z} \\frac{\\partial C}{\\partial a} \\\\\n  &= I \\Sigma'(z) \\frac{\\partial C}{\\partial a} \\\\\n  &= \\frac{\\partial C}{\\partial a} \\odot \\sigma'(z) = \\frac{\\partial C}{\\partial a} \\odot a(1 - a) \\\\\n  \\text{Equating the two} \\\\\n  (a - y) &= \\frac{\\partial C}{\\partial a} \\odot \\sigma'(z) \\\\\n  \\text{Abusing division notation..} \\\\\n  \\frac{\\partial C}{\\partial a} &= \\frac{a - y}{a(1 - a)} \\\\\n  \\implies C\n  &= -\\left[y \\cdot \\ln (a) + (1 - y) \\cdot \\ln (1 - a) \\right] + \\text{const} \\\\\n  &&\\text{as required} \\\\\n\\end{align*}\n\\]\n\nAnother factor that may inhibit learning is the presence of the \\(x_j\\) term in Equation (61). Because of this term, when an input \\(x_j\\) is near to zero, the corresponding weight \\(w_j\\) will learn slowly.\n\n\nIn the final layer of a multi-layer NN, \\(x_j = a_j^{L-1}\\) (i.e. the activation of the neurons in the previous layer).\nEquation 61 refers to: \\(\\frac{\\partial C}{\\partial w^L} = \\delta^L {a^{L-1}}^T\\)\n\nIt is not possible to elimnate this term through a clever choice of cost function because of the weighted input equation: \\(z = w \\cdot a + b\\). Because of the linearity in the derivative operator, \\(a\\) will always appear as a factor when differentiating wrt \\(w\\).\n\n\n\n\nInstead of using sigmoid as the activation function in the output layer, we can use softmax:\n\\[\na_j^L = \\text{softmax}(z, j) = \\frac{\\exp(z_j^L)}{\\sum_k \\exp(z_k^L)}\n\\]\n\nThis can be interpreted as a probability distribution because it obeys the two laws:\n\n\\(a_j^L &gt; 0\\) because \\(\\exp(z) &gt; 0\\)\n\\(\\sum_j a_j^L = \\sum_j \\frac{\\exp(z_j^L)}{\\sum_k \\exp(z_k^L)} = \\frac{\\sum_j \\exp(z_j^L)}{\\sum_k \\exp(z_k^L)} = 1\\)\n\n\nNote: It is obvious that the output of a sigmoid layer will not form a probability distribution, because the outputs aren’t rescaled.\nThe associated cost function is the log-likelihood function:\n\\[\nC_{LL,x} = -\\ln (y^T a^L) \\equiv - y^T \\ln (a^L)\n\\]\nwhere \\(y\\) is the one-hot encoded class for the corresponing input \\(x\\).\n\n\n\nMonoticity of softmax\n\nConsider the derivative of the softmax function wrt an input:\n\\[\n\\begin{align*}\n  \\frac{\\partial a_j^L}{\\partial z_k^L}\n  &= \\frac{\\partial}{\\partial z_k^L} \\left\\{ \\exp(z_j^L) \\left(\\sum_i \\exp(z_i^L)\\right)^{-1} \\right\\} \\\\\n  %%% Consider j not equal to k\n  \\text{for } j \\ne k \\\\\n  &= - \\exp(z_j^L) \\exp(z_k^L) \\left(\\sum_i \\exp(z_i^L)\\right)^{-2} \\\\\n  &= - a_j^L (1 - a_k^L)\\\\  \n  & &lt; 0 \\\\\n  %%% Consider j=k\n  \\text{for } j = k \\\\\n  &= \\exp(z_j^L) \\left(\\sum_i \\exp(z_i^L)\\right)^{-1} - \\exp(2z_j^L) \\left(\\sum_i \\exp(z_i^L)\\right)^{-2} \\\\\n  &= a_j^L - {a_j^L}^2 = a_j^L (1 - a_j^L) \\\\\n  &= \\exp(z_j^L) \\left( \\sum_{i \\ne j} \\exp(z_i^L) \\right) \\left(\\sum_i \\exp(z_i^L)\\right)^{-2} \\\\\n  % &&gt; 0\n\\end{align*}\n\\]\nHence, increasing \\(z_k^L\\) is guaranteed to increase the corresponding output activation \\(a_k^L\\) while decreasing the others.\n\nNon-locality of softmax\n\nA consequence of the denominator of the softmax function is that the output depends on all the weighted inputs (unlike the sigmoid).\n\nInverting the softmax layer\n\nTo find the weighted input \\(z_j^L\\) given the output activations \\(\\vec{a}_L\\):\n\\[\n\\begin{align*}\n  a_j^L &= \\frac{\\exp(z_j^L)}{\\sum_k \\exp(z_k^L)} \\\\\n  \\iff z_j^L &= \\ln(a_j^L) - \\ln \\left(\\sum_k \\exp(z_k^L)\\right) \\\\\n  &= \\ln(a_j^L) + C \\ \\text{, for some constant } C\n\\end{align*}\n\\]\n\nAvoiding learning slowdown\n\nConsider the derivative of the cost function wrt the weights:\n\\[\n\\begin{align*}\n  \\frac{\\partial C_{LL,x}}{\\partial w^L}\n  &= \\frac{\\partial z^L}{\\partial w^L} \\frac{\\partial a^L}{\\partial z^L} \\frac{\\partial C}{\\partial a^L} \\\\\n  &= ({a^{L-1}}^T \\otimes I) \\frac{\\partial a^L}{\\partial z^L} \\cdot - \\frac{1}{a^L} \\cdot y \\\\\n  &= \\begin{cases}\n    ({a^{L-1}}^T \\otimes I)\n  \\end{cases}\n\\end{align*}\n\\]\nTODO above..\n\nWhere does the “softmax” name come from?\n\nConsider:\n\\[\n\\begin{align*}\n  &\\lim_{c \\to \\inf} \\frac{\\exp cz_j}{\\sum_k \\exp c z_k} \\\\\n  = &\\lim_{c \\to \\inf} \\left(1 + \\sum_{k \\forall k \\ne j} \\exp c(z_k - z_j) \\right)^{-1} \\\\\n  \\to & \\begin{cases}\n    0 & \\text{if } z_j &lt; z_k \\forall k \\\\\n    1 & \\text{if } z_j &gt; z_k \\forall k \\\\\n    \\frac{1}{m} & \\text{if } z_j = z_i \\forall k : |k| = m\n  \\end{cases}\n\\end{align*}\n\\]\nThis is equivalent to the argmax function (if there are no \\(z_k = z_j\\)).\n\nBackpropagation with softmax and the log-likelihood cost\n\nTo find an expression for \\(\\delta^L = \\frac{\\partial C}{\\partial z^L}\\):\n\\[\n\\begin{align*}\n  \\delta^L &= \\frac{\\partial C}{\\partial z^L} \\\\\n  &= \\frac{\\partial a^L}{\\partial z^L} \\frac{\\partial C}{\\partial a^L} \\\\\n  &= \\frac{\\partial a^L}{\\partial z^L} \\cdot - \\frac{1}{a_L} \\odot y \\\\\n  \\text{Note } y_j = 1 \\\\\n  &= \\begin{cases}\n    a_j^L - 1 & \\text{if } k = j \\\\\n    1 - a_k^L & \\text{otherwise}\\\\\n  \\end{cases}\n\\end{align*}\n\\]\nSlightly confused by the textbook.\n\n\n\n\n\nearly stopping is used to stop training when the classification accuracy of the validation set plateus.\nhold out method is when a validation set is used to tune the hyperparameters of the model.\nUse more training data to prevent overfitting\n\n\n\n\n\n\n\\[\nC = C_0 +\\frac{\\lambda}{2n} \\sum_w w^2\n\\]\nwhere \\(\\lambda\\) is the regularization parameter - larger \\(\\lambda\\) means larger weights will be penalised more. This leads to the following weight update equation:\n\\[\n\\begin{align*}\n  \\text{Gradient descent:}\\\\\n  w &\\to \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\eta \\frac{\\partial C_0}{\\partial w} \\\\\n  \\text{SGD:} \\\\\n  w &\\to \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\frac{\\eta}{m} \\sum_m \\frac{\\partial C_{0,x}}{\\partial w} \\\\\n\\end{align*}\n\\]\nHence, on each update, the weight is rescaled to be smaller, unless this is a detriment to the unregularized cost function. Note in SGD, the weight decay has the same factor of \\(1/n\\), but the unregularized cost function is averaged as normal.\n\nNote that the update equation for the biases doesn’t change, because the regularization term doesn’t include the biases! Whether or not to incldue biases is dependent on the network. Genereally it doesn’t affect the accuracy so much, however large biases can lead to saturation which may be desirable in some cases.\nNote that when the training examples increases, the regularization patameter must be increased as well to keep \\(\\frac{\\eta \\lambda}{n}\\) the same\nWithout regularization, SGD can easily be stuck in a local minima. Intuitively, this can be because when the weight vectors are large, small changes to them will still point them in a similar direction to before - i.e. not all directions will be explored in the cost function landscape. Hence, regularization resolves this by keeping the weight vectors small, so that small changes will cause larger changes in direction, resulting in SGD not being stuck in local minima so often.\n\n\n\n\n\\[\nC = C_0 + \\frac{\\lambda}{n} \\sum_w |w|\n\\]\nThis gives the update equation:\n\\[\nw \\to w' = w - \\frac{\\eta \\lambda}{n} \\text{sgn}(w) - \\eta \\frac{\\partial C_0}{\\partial w}\n\\]\nComparing this to L2 regularization, the differences are:\n\nWhen the weights are large, L2 will penalise more\nWhen the weights are small, L1 will drive them to 0\n\nThis gives the effect that L1 regularization will tend to have a relatively small number of high-importnace connections, while the other weigths are driven to 0.\n\n\n\nDuring the training procedure, we add extra steps:\n\ndeactivate out half of the neurons in the hidden layer\nperform fwd and backprop\nreintroduce the neurons and repeat\n\nAfter training, all the nerons are kept active, but the weights in the connected layer are halved (because previously it was learning on only half as many neurons).\nHeuristically, this works because dropout is analogous to ensembling multiple NNs. Another explanation is:\n\n“This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.”\n\n\n\n\n\n\n\nAlt text\n\n\nThis graph shows that as the training set size increases, the accuracy increases. However gathering more data can be expensive. Instead, artificially increasing the training set size can make vast improvements:\n\nRotating the images by some amount (if arbitrarily large rotations are allowed, then a 6 may look like a 9!)\n“elastic distortions” aim to mimic the variance added by real human handwriting\n\nNote, the converse is also true: reducing the noise in the training data can be more beneficial in e.g. signal processing tasks such as speech recognition.\nKey takeaway: Improving training data is just as important as improving the algorithms.\n\n\n\n\nHow do our machine learning algorithms perform in the limit of very large data sets?\n\nTbh, I’m not sure what is the correct extrapolation. Currently, LLMs continue to improve with more data. At some point, the data will be large enought that it encompasses all possibilties, hence the accuracy should reach 100% asymptotically (perhaps only asmyptotically, because one could construct an example that catches out the model?)\n\n\n\n\nMotivation: Consider intialising the weights and biases: \\(w,b \\sim \\mathcal{N}(\\mu=0, \\sigma^2=1)\\). Immediately after initialisation, the weighted input to neurons to any layer \\(z^l = w^l \\cdot a^{l-1} + b^l\\), so \\(z\\) will be the sum of Guassians which is also Guassian: \\(z^l \\sim \\mathcal{N}(\\mu=0, \\sigma^2=n + 1)\\), where \\(n\\) is the number of non-zero activations in the previous layer (the \\(+1\\) comes from the bias). Hence, \\(z^l\\) has very large variance, which will make it more likely for that neuron to saturate and learn more slowly.\nA better intialisation scheme is to set the variance of the weights: \\(w \\sim \\mathcal{N}(0, 1/n_\\text{in})\\).\nInterestingly, the initialisation of the bias doesn’t matter so much (as long as it doesn’t cause early saturation).\nNote: weight initialisation improves the rate of convergence to an optimal set of weights and biases; however, it doesn’t necessarly improve the accuracy of the model. (in Chap 4, better initialisations do actually enable a other methods to improve the performance).\n\nConnecting regularization and the improved method of weight initialzation\n\nRecall: Weight update in stochastic gradient descent\n\\[\nw \\to w' = \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\frac{\\eta}{m} \\sum_m \\frac{\\partial C_{0,x}}{\\partial w} \\\\\n\\]\nSuppose we are using the original approach to weight initialization. A wavy argument can be formulated to show that L2 regularization leads to the improved initialization scheme:\n\nIn the first few epochs, \\(\\frac{\\partial C}{\\partial w}\\) will be small because the neurons are saturated. Hence, assuming \\(\\lambda\\) is large enough, the weight update equation simplifies to:\n\n\\[\n\\begin{align*}\n  \\text{Per SGD update:} \\\\\n  w \\to w'& = \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w \\\\\n  \\text{Per epoch:} \\\\\n  w \\to w'& = \\left(1 - \\frac{\\eta \\lambda / m}{n/m} \\right)^{\\frac{n}{m}} w\n\\end{align*}\n\\]\n\nFor \\(n/m \\gg \\eta \\lambda / m\\), the weight decay per epoch simplifies to: \\(w' = \\exp - (\\eta \\lambda / m)\\).\n(?? Not sure about this step) Supposing \\(\\lambda\\) is not too large, weight decays will tail off when the \\(|w| \\sim 1/\\sqrt{n}\\)\n\n\n\n\nIt can be overwhelming to choos hyperparemeters, because you don’t know where to start.\n\n\nAs a baseline, aim to get any non-trivial learning by performing better than chance. The idea is to meaningfully reduce the problem so that you can feedback quick enough for you to iterate.\n\nReduce the training set classes (e.g. 0s and 1s only)\n\nSpeeds up training significantly\nThis can be useful for debugging as well\nNote that changing the size of training set will require modification to the relevant hyper-parameters\n\nReduce validation set size to speed up evaluation\nStart off with no hidden layer\n\n\n\n\n\nPick the learning rate \\(\\eta\\) by monitoring the training cost.\nUse early stopping to determine the number of epochs\n\nHowever, in the early stages, don’t use early stopping, because increased epochs can help monitor regularization performance\nA good rule of thumb is no-improvement-in-ten-rule, i.e. stop if no improvement after 10 epochs. This can be made more lenient as other parameters are well adjusted.\nOther methods exist, which compromise achieving high validation accuracies for not training too long.\n\nLearning rate schedule is used to vary the learning rate\n\nOne scheme is to hold the learning rate constant until the validation accuracy starts to worsen, in which case the learning rate should be decreased by a factor of 2 or 10.\nTermination can be done when the learning rate has decrease by a factor of 100 or 1000.\n\nTo determine the regularization parameter \\(\\lambda\\), it is worth setting it to 0 and choosing the learning rate. Then set \\(\\lambda=1\\) and experiment\nUse the validation accuracy to pick regularization hyperparemeter, mini-batch size, network parameters.\n\n\nIt’s tempting to use gradient descent to try to learn good values for hyper-paremeters\n\nPotentially this isn’t the best idea because the landscae isn’t broadly convex. Gradient descent is probably too slow for this. Alternative automated methods are:\n\ngrid search\nbayesian approach\n\n\n\n\n\n\n\n\n\nTaylor expansion gives\n\\[\n\\begin{align*}\n  C(w + \\Delta w) & \\approx C(w) + \\nabla C \\cdot \\Delta w + \\frac{1}{2} \\Delta w^T H \\Delta w \\\\\n  \\text{Assuming H is +ve definite,} \\\\\n  \\text{The minimum is found as} \\\\\n  \\implies \\Delta w &= - H^{-1} \\nabla C \\\\\n  \\text{Giving the update} \\\\\n  w \\to w' &= w - H^{-1} \\nabla C \\\\\n\\end{align*}\n\\]\nAdvantages:\n\nconverges faster than 1st order approximation\nversions of backprop exist to calculate the Hessian\n\nDisadvantage: Too expensive and space heavy\n\n\n\nThe Hessian adds information about the way the gradient is changing. The momentum based methods aims to do this by augmenting with a velocity term:\n\\[\n\\begin{align*}\n  v \\to v' &= \\mu v - \\eta \\nabla C \\\\\n  w \\to w' &= w + v' \\\\\n\\end{align*}\n\\]\n\\(\\mu\\) controls the amount of velocity in the system (momentum co-efficient, though intuitively it controls the friction). \\(\\mu = 1 \\implies\\) there is no friction, hence the system will roll down a hill and may overshoot. For \\(\\mu &gt; 0 \\implies\\) the friction is infinte, i.e. the system reduces to the original udpate equations.\nNote: for \\(\\mu &gt; 1\\), energy will be added to the system, which could result in instability. For \\(\\mu &lt; 0\\), gradient may traverse uphill instead.\n\n\n\n\nConjugate gradient descent\nBFGS / L-BFGS (the latter being the limited memory implentation)\nWhat about Barzilai-Borwein?\nPromising: Nesterov’s accelerated gradient technique (though it may be outdated now)\n\n\n\n\n\n\n\n\\[\n\\sigma(z) = \\frac{1}{2}(1 + \\tanh(z/2))\n\\]\nHence, it is simply a linear transformation. Note that the output of tanh is \\(\\in (-1, 1)\\), which can make interpretation slightly different. Potentially, tanh neurons can be better for learning because the output activation can be any sign. Consider BP4: \\(\\partial C / \\partial w = \\delta^l {a^{l-1}}^T\\); so for a sigmoid neuron, all the activations will be positive, which will mean that a whole row of weights \\(w_j^l\\) will increase / decrease depending on the sign of \\(\\delta_j^l\\), rather than increasing / decreasing independently of each other. Moreover, tanh is an odd function, hence the activations will on average be equally balanced between +ve and -ve.\n\n\n\nrectified linear unit\n\\[\n\\text{ReLu} = \\max(0, z)\n\\]\nReLu doesn’t suffer from the saturation problem. On the flip side, when \\(z &lt; 0\\), the gradient is 0, so the neuron stops learning entirely.\nNote: ReLu requires a different initialisation scheme. This article is very helpful introduction.",
    "crumbs": [
      "Notes",
      "Improving Learning"
    ]
  },
  {
    "objectID": "notes/3-Improving_Learning.html#the-cross-entropy-cost-function",
    "href": "notes/3-Improving_Learning.html#the-cross-entropy-cost-function",
    "title": "Improving the way neural networks learn",
    "section": "",
    "text": "Motivation: As seen in Chap 2, a neuron learns very slowly if the activation function is saturated (i.e. \\(z\\) is at an extreme such that the gradient \\(\\sigma'(z)\\) is too small to cause any substantial change in the update step). Consider the weights in the final layer:\n\\[\n\\begin{align*}\n\\frac{\\partial C_{MSE}}{\\partial w^L} &= \\delta^L {a^{L-1}}^T \\\\\n&= (a^L - y) \\odot \\sigma'(z^L) {a^{L-1}}^T \\\\\n& \\propto \\sigma'(z^L)\n\\end{align*}\n\\]\nCross-Entropy loss resolves this by removing the dependency on the gradient of the sigmoid activation function. The CE loss function \\(C_{CE}\\) is given by:\n\\[\n\\begin{align*}\n  C_{CE} &= \\frac{1}{n} \\sum_x C_{CE,x} \\\\\n  \\text{where:}\\\\\n  C_{CE,x} &= - \\sum_j \\left[y_j \\ln a_j^L + (1 - y_j) \\ln(1 - a_j^L) \\right] \\\\\n  &= -y \\cdot \\ln (a^L) - (1 - y) \\cdot \\ln (1 - a^L)\n\\end{align*}\n\\]\nTo find the gradient wrt to a weight\n\\[\n\\begin{align*}\n\\frac{\\partial C_{CE,x}}{\\partial w^L}\n&= \\frac{\\partial z^L}{\\partial w^L} \\frac{\\partial a^L}{\\partial z^L} \\frac{\\partial C_{CE,x}}{\\partial a^L} \\\\\n\\text{Note } a^L = \\sigma (z^L) \\\\\n&= ({a^{L-1}}^T \\otimes I ) \\ \\sigma'(z^L) \\cdot \\left[ -y \\cdot \\frac{1}{\\sigma (z^L)} +(1 - y) \\cdot \\frac{1}{(1 - \\sigma(z^L))}\\right] \\\\\n&= ({a^{L-1}}^T \\otimes I ) \\ \\sigma'(z^L) \\cdot \\left[ \\frac{\\sigma(z^L) - y}{\\sigma(z^L)(1 - \\sigma(z^L))}\\right] \\\\\n\\text{Note } \\sigma' = \\sigma(1 - \\sigma) \\\\\n&= \\left({a^{L-1}}^T \\otimes I \\right) \\cdot \\left( \\sigma(z^L) - y \\right) \\\\\n&= \\left( \\sigma(z^L) - y \\right) {a^{L-1}}^T \\\\\n\\end{align*}\n\\]\ni.e. the gradient is proprtional to the error - this makes much more sense, because now the network will learn quicker if the error is larger (which is much more human like as well). (Note: vector-matrix derivative and Kronecher product, though the last line is dubious.)\n\n\n\nThe right form of cross-entropy\n\nConsider the incorrect form; \\(- [a \\ln y + (1 - a) \\ln (1 - y)]\\). In a classification problem, \\(y \\in \\{0,1\\}\\) and in either case, the cost will be very large and positive: \\(+\\inf \\times \\{a, 1-a\\}\\).\n\nRegression problems\n\nIn regression problems, \\(y \\in [0,1]\\). The cross entropy will still be minimised when \\(a(z) = y\\) by Gibb’s inequality.\n\nUsing the quadratic cost when we have linear neurons in the output layer\n\nThen we have:\n\\[\n\\begin{align*}\n\\frac{\\partial C_{MSE,x}}{\\partial w^L} &= \\delta^L {a^{L-1}}^T \\\\\n&= (a^L - y) {a^{L-1}}^T \\\\\n\\end{align*}\n\\]\nThis resolves the saturation issue in the final layer, however the issue still remains in the previous layers.\n\n\n\nWorking backwards, we can derive the CE loss by supposing that we want to satisfy this differential equation for the bias:\n\\[\n\\begin{align*}\n  \\frac{\\partial C}{\\partial b} &= (a - y) \\\\\n  \\text{Using the chain rule} \\\\\n  &= \\frac{\\partial z}{\\partial b} \\frac{\\partial a}{\\partial z} \\frac{\\partial C}{\\partial a} \\\\\n  &= I \\Sigma'(z) \\frac{\\partial C}{\\partial a} \\\\\n  &= \\frac{\\partial C}{\\partial a} \\odot \\sigma'(z) = \\frac{\\partial C}{\\partial a} \\odot a(1 - a) \\\\\n  \\text{Equating the two} \\\\\n  (a - y) &= \\frac{\\partial C}{\\partial a} \\odot \\sigma'(z) \\\\\n  \\text{Abusing division notation..} \\\\\n  \\frac{\\partial C}{\\partial a} &= \\frac{a - y}{a(1 - a)} \\\\\n  \\implies C\n  &= -\\left[y \\cdot \\ln (a) + (1 - y) \\cdot \\ln (1 - a) \\right] + \\text{const} \\\\\n  &&\\text{as required} \\\\\n\\end{align*}\n\\]\n\nAnother factor that may inhibit learning is the presence of the \\(x_j\\) term in Equation (61). Because of this term, when an input \\(x_j\\) is near to zero, the corresponding weight \\(w_j\\) will learn slowly.\n\n\nIn the final layer of a multi-layer NN, \\(x_j = a_j^{L-1}\\) (i.e. the activation of the neurons in the previous layer).\nEquation 61 refers to: \\(\\frac{\\partial C}{\\partial w^L} = \\delta^L {a^{L-1}}^T\\)\n\nIt is not possible to elimnate this term through a clever choice of cost function because of the weighted input equation: \\(z = w \\cdot a + b\\). Because of the linearity in the derivative operator, \\(a\\) will always appear as a factor when differentiating wrt \\(w\\).",
    "crumbs": [
      "Notes",
      "Improving Learning"
    ]
  },
  {
    "objectID": "notes/3-Improving_Learning.html#softmax",
    "href": "notes/3-Improving_Learning.html#softmax",
    "title": "Improving the way neural networks learn",
    "section": "",
    "text": "Instead of using sigmoid as the activation function in the output layer, we can use softmax:\n\\[\na_j^L = \\text{softmax}(z, j) = \\frac{\\exp(z_j^L)}{\\sum_k \\exp(z_k^L)}\n\\]\n\nThis can be interpreted as a probability distribution because it obeys the two laws:\n\n\\(a_j^L &gt; 0\\) because \\(\\exp(z) &gt; 0\\)\n\\(\\sum_j a_j^L = \\sum_j \\frac{\\exp(z_j^L)}{\\sum_k \\exp(z_k^L)} = \\frac{\\sum_j \\exp(z_j^L)}{\\sum_k \\exp(z_k^L)} = 1\\)\n\n\nNote: It is obvious that the output of a sigmoid layer will not form a probability distribution, because the outputs aren’t rescaled.\nThe associated cost function is the log-likelihood function:\n\\[\nC_{LL,x} = -\\ln (y^T a^L) \\equiv - y^T \\ln (a^L)\n\\]\nwhere \\(y\\) is the one-hot encoded class for the corresponing input \\(x\\).\n\n\n\nMonoticity of softmax\n\nConsider the derivative of the softmax function wrt an input:\n\\[\n\\begin{align*}\n  \\frac{\\partial a_j^L}{\\partial z_k^L}\n  &= \\frac{\\partial}{\\partial z_k^L} \\left\\{ \\exp(z_j^L) \\left(\\sum_i \\exp(z_i^L)\\right)^{-1} \\right\\} \\\\\n  %%% Consider j not equal to k\n  \\text{for } j \\ne k \\\\\n  &= - \\exp(z_j^L) \\exp(z_k^L) \\left(\\sum_i \\exp(z_i^L)\\right)^{-2} \\\\\n  &= - a_j^L (1 - a_k^L)\\\\  \n  & &lt; 0 \\\\\n  %%% Consider j=k\n  \\text{for } j = k \\\\\n  &= \\exp(z_j^L) \\left(\\sum_i \\exp(z_i^L)\\right)^{-1} - \\exp(2z_j^L) \\left(\\sum_i \\exp(z_i^L)\\right)^{-2} \\\\\n  &= a_j^L - {a_j^L}^2 = a_j^L (1 - a_j^L) \\\\\n  &= \\exp(z_j^L) \\left( \\sum_{i \\ne j} \\exp(z_i^L) \\right) \\left(\\sum_i \\exp(z_i^L)\\right)^{-2} \\\\\n  % &&gt; 0\n\\end{align*}\n\\]\nHence, increasing \\(z_k^L\\) is guaranteed to increase the corresponding output activation \\(a_k^L\\) while decreasing the others.\n\nNon-locality of softmax\n\nA consequence of the denominator of the softmax function is that the output depends on all the weighted inputs (unlike the sigmoid).\n\nInverting the softmax layer\n\nTo find the weighted input \\(z_j^L\\) given the output activations \\(\\vec{a}_L\\):\n\\[\n\\begin{align*}\n  a_j^L &= \\frac{\\exp(z_j^L)}{\\sum_k \\exp(z_k^L)} \\\\\n  \\iff z_j^L &= \\ln(a_j^L) - \\ln \\left(\\sum_k \\exp(z_k^L)\\right) \\\\\n  &= \\ln(a_j^L) + C \\ \\text{, for some constant } C\n\\end{align*}\n\\]\n\nAvoiding learning slowdown\n\nConsider the derivative of the cost function wrt the weights:\n\\[\n\\begin{align*}\n  \\frac{\\partial C_{LL,x}}{\\partial w^L}\n  &= \\frac{\\partial z^L}{\\partial w^L} \\frac{\\partial a^L}{\\partial z^L} \\frac{\\partial C}{\\partial a^L} \\\\\n  &= ({a^{L-1}}^T \\otimes I) \\frac{\\partial a^L}{\\partial z^L} \\cdot - \\frac{1}{a^L} \\cdot y \\\\\n  &= \\begin{cases}\n    ({a^{L-1}}^T \\otimes I)\n  \\end{cases}\n\\end{align*}\n\\]\nTODO above..\n\nWhere does the “softmax” name come from?\n\nConsider:\n\\[\n\\begin{align*}\n  &\\lim_{c \\to \\inf} \\frac{\\exp cz_j}{\\sum_k \\exp c z_k} \\\\\n  = &\\lim_{c \\to \\inf} \\left(1 + \\sum_{k \\forall k \\ne j} \\exp c(z_k - z_j) \\right)^{-1} \\\\\n  \\to & \\begin{cases}\n    0 & \\text{if } z_j &lt; z_k \\forall k \\\\\n    1 & \\text{if } z_j &gt; z_k \\forall k \\\\\n    \\frac{1}{m} & \\text{if } z_j = z_i \\forall k : |k| = m\n  \\end{cases}\n\\end{align*}\n\\]\nThis is equivalent to the argmax function (if there are no \\(z_k = z_j\\)).\n\nBackpropagation with softmax and the log-likelihood cost\n\nTo find an expression for \\(\\delta^L = \\frac{\\partial C}{\\partial z^L}\\):\n\\[\n\\begin{align*}\n  \\delta^L &= \\frac{\\partial C}{\\partial z^L} \\\\\n  &= \\frac{\\partial a^L}{\\partial z^L} \\frac{\\partial C}{\\partial a^L} \\\\\n  &= \\frac{\\partial a^L}{\\partial z^L} \\cdot - \\frac{1}{a_L} \\odot y \\\\\n  \\text{Note } y_j = 1 \\\\\n  &= \\begin{cases}\n    a_j^L - 1 & \\text{if } k = j \\\\\n    1 - a_k^L & \\text{otherwise}\\\\\n  \\end{cases}\n\\end{align*}\n\\]\nSlightly confused by the textbook.",
    "crumbs": [
      "Notes",
      "Improving Learning"
    ]
  },
  {
    "objectID": "notes/3-Improving_Learning.html#overfitting-and-regularization",
    "href": "notes/3-Improving_Learning.html#overfitting-and-regularization",
    "title": "Improving the way neural networks learn",
    "section": "",
    "text": "early stopping is used to stop training when the classification accuracy of the validation set plateus.\nhold out method is when a validation set is used to tune the hyperparameters of the model.\nUse more training data to prevent overfitting",
    "crumbs": [
      "Notes",
      "Improving Learning"
    ]
  },
  {
    "objectID": "notes/3-Improving_Learning.html#regularization",
    "href": "notes/3-Improving_Learning.html#regularization",
    "title": "Improving the way neural networks learn",
    "section": "",
    "text": "\\[\nC = C_0 +\\frac{\\lambda}{2n} \\sum_w w^2\n\\]\nwhere \\(\\lambda\\) is the regularization parameter - larger \\(\\lambda\\) means larger weights will be penalised more. This leads to the following weight update equation:\n\\[\n\\begin{align*}\n  \\text{Gradient descent:}\\\\\n  w &\\to \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\eta \\frac{\\partial C_0}{\\partial w} \\\\\n  \\text{SGD:} \\\\\n  w &\\to \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\frac{\\eta}{m} \\sum_m \\frac{\\partial C_{0,x}}{\\partial w} \\\\\n\\end{align*}\n\\]\nHence, on each update, the weight is rescaled to be smaller, unless this is a detriment to the unregularized cost function. Note in SGD, the weight decay has the same factor of \\(1/n\\), but the unregularized cost function is averaged as normal.\n\nNote that the update equation for the biases doesn’t change, because the regularization term doesn’t include the biases! Whether or not to incldue biases is dependent on the network. Genereally it doesn’t affect the accuracy so much, however large biases can lead to saturation which may be desirable in some cases.\nNote that when the training examples increases, the regularization patameter must be increased as well to keep \\(\\frac{\\eta \\lambda}{n}\\) the same\nWithout regularization, SGD can easily be stuck in a local minima. Intuitively, this can be because when the weight vectors are large, small changes to them will still point them in a similar direction to before - i.e. not all directions will be explored in the cost function landscape. Hence, regularization resolves this by keeping the weight vectors small, so that small changes will cause larger changes in direction, resulting in SGD not being stuck in local minima so often.\n\n\n\n\n\\[\nC = C_0 + \\frac{\\lambda}{n} \\sum_w |w|\n\\]\nThis gives the update equation:\n\\[\nw \\to w' = w - \\frac{\\eta \\lambda}{n} \\text{sgn}(w) - \\eta \\frac{\\partial C_0}{\\partial w}\n\\]\nComparing this to L2 regularization, the differences are:\n\nWhen the weights are large, L2 will penalise more\nWhen the weights are small, L1 will drive them to 0\n\nThis gives the effect that L1 regularization will tend to have a relatively small number of high-importnace connections, while the other weigths are driven to 0.\n\n\n\nDuring the training procedure, we add extra steps:\n\ndeactivate out half of the neurons in the hidden layer\nperform fwd and backprop\nreintroduce the neurons and repeat\n\nAfter training, all the nerons are kept active, but the weights in the connected layer are halved (because previously it was learning on only half as many neurons).\nHeuristically, this works because dropout is analogous to ensembling multiple NNs. Another explanation is:\n\n“This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.”\n\n\n\n\n\n\n\nAlt text\n\n\nThis graph shows that as the training set size increases, the accuracy increases. However gathering more data can be expensive. Instead, artificially increasing the training set size can make vast improvements:\n\nRotating the images by some amount (if arbitrarily large rotations are allowed, then a 6 may look like a 9!)\n“elastic distortions” aim to mimic the variance added by real human handwriting\n\nNote, the converse is also true: reducing the noise in the training data can be more beneficial in e.g. signal processing tasks such as speech recognition.\nKey takeaway: Improving training data is just as important as improving the algorithms.\n\n\n\n\nHow do our machine learning algorithms perform in the limit of very large data sets?\n\nTbh, I’m not sure what is the correct extrapolation. Currently, LLMs continue to improve with more data. At some point, the data will be large enought that it encompasses all possibilties, hence the accuracy should reach 100% asymptotically (perhaps only asmyptotically, because one could construct an example that catches out the model?)",
    "crumbs": [
      "Notes",
      "Improving Learning"
    ]
  },
  {
    "objectID": "notes/3-Improving_Learning.html#weight-initialization",
    "href": "notes/3-Improving_Learning.html#weight-initialization",
    "title": "Improving the way neural networks learn",
    "section": "",
    "text": "Motivation: Consider intialising the weights and biases: \\(w,b \\sim \\mathcal{N}(\\mu=0, \\sigma^2=1)\\). Immediately after initialisation, the weighted input to neurons to any layer \\(z^l = w^l \\cdot a^{l-1} + b^l\\), so \\(z\\) will be the sum of Guassians which is also Guassian: \\(z^l \\sim \\mathcal{N}(\\mu=0, \\sigma^2=n + 1)\\), where \\(n\\) is the number of non-zero activations in the previous layer (the \\(+1\\) comes from the bias). Hence, \\(z^l\\) has very large variance, which will make it more likely for that neuron to saturate and learn more slowly.\nA better intialisation scheme is to set the variance of the weights: \\(w \\sim \\mathcal{N}(0, 1/n_\\text{in})\\).\nInterestingly, the initialisation of the bias doesn’t matter so much (as long as it doesn’t cause early saturation).\nNote: weight initialisation improves the rate of convergence to an optimal set of weights and biases; however, it doesn’t necessarly improve the accuracy of the model. (in Chap 4, better initialisations do actually enable a other methods to improve the performance).\n\nConnecting regularization and the improved method of weight initialzation\n\nRecall: Weight update in stochastic gradient descent\n\\[\nw \\to w' = \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\frac{\\eta}{m} \\sum_m \\frac{\\partial C_{0,x}}{\\partial w} \\\\\n\\]\nSuppose we are using the original approach to weight initialization. A wavy argument can be formulated to show that L2 regularization leads to the improved initialization scheme:\n\nIn the first few epochs, \\(\\frac{\\partial C}{\\partial w}\\) will be small because the neurons are saturated. Hence, assuming \\(\\lambda\\) is large enough, the weight update equation simplifies to:\n\n\\[\n\\begin{align*}\n  \\text{Per SGD update:} \\\\\n  w \\to w'& = \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w \\\\\n  \\text{Per epoch:} \\\\\n  w \\to w'& = \\left(1 - \\frac{\\eta \\lambda / m}{n/m} \\right)^{\\frac{n}{m}} w\n\\end{align*}\n\\]\n\nFor \\(n/m \\gg \\eta \\lambda / m\\), the weight decay per epoch simplifies to: \\(w' = \\exp - (\\eta \\lambda / m)\\).\n(?? Not sure about this step) Supposing \\(\\lambda\\) is not too large, weight decays will tail off when the \\(|w| \\sim 1/\\sqrt{n}\\)",
    "crumbs": [
      "Notes",
      "Improving Learning"
    ]
  },
  {
    "objectID": "notes/3-Improving_Learning.html#how-to-choose-a-neural-networks-hyper-parameters",
    "href": "notes/3-Improving_Learning.html#how-to-choose-a-neural-networks-hyper-parameters",
    "title": "Improving the way neural networks learn",
    "section": "",
    "text": "It can be overwhelming to choos hyperparemeters, because you don’t know where to start.\n\n\nAs a baseline, aim to get any non-trivial learning by performing better than chance. The idea is to meaningfully reduce the problem so that you can feedback quick enough for you to iterate.\n\nReduce the training set classes (e.g. 0s and 1s only)\n\nSpeeds up training significantly\nThis can be useful for debugging as well\nNote that changing the size of training set will require modification to the relevant hyper-parameters\n\nReduce validation set size to speed up evaluation\nStart off with no hidden layer\n\n\n\n\n\nPick the learning rate \\(\\eta\\) by monitoring the training cost.\nUse early stopping to determine the number of epochs\n\nHowever, in the early stages, don’t use early stopping, because increased epochs can help monitor regularization performance\nA good rule of thumb is no-improvement-in-ten-rule, i.e. stop if no improvement after 10 epochs. This can be made more lenient as other parameters are well adjusted.\nOther methods exist, which compromise achieving high validation accuracies for not training too long.\n\nLearning rate schedule is used to vary the learning rate\n\nOne scheme is to hold the learning rate constant until the validation accuracy starts to worsen, in which case the learning rate should be decreased by a factor of 2 or 10.\nTermination can be done when the learning rate has decrease by a factor of 100 or 1000.\n\nTo determine the regularization parameter \\(\\lambda\\), it is worth setting it to 0 and choosing the learning rate. Then set \\(\\lambda=1\\) and experiment\nUse the validation accuracy to pick regularization hyperparemeter, mini-batch size, network parameters.\n\n\nIt’s tempting to use gradient descent to try to learn good values for hyper-paremeters\n\nPotentially this isn’t the best idea because the landscae isn’t broadly convex. Gradient descent is probably too slow for this. Alternative automated methods are:\n\ngrid search\nbayesian approach",
    "crumbs": [
      "Notes",
      "Improving Learning"
    ]
  },
  {
    "objectID": "notes/3-Improving_Learning.html#other-techniques",
    "href": "notes/3-Improving_Learning.html#other-techniques",
    "title": "Improving the way neural networks learn",
    "section": "",
    "text": "Taylor expansion gives\n\\[\n\\begin{align*}\n  C(w + \\Delta w) & \\approx C(w) + \\nabla C \\cdot \\Delta w + \\frac{1}{2} \\Delta w^T H \\Delta w \\\\\n  \\text{Assuming H is +ve definite,} \\\\\n  \\text{The minimum is found as} \\\\\n  \\implies \\Delta w &= - H^{-1} \\nabla C \\\\\n  \\text{Giving the update} \\\\\n  w \\to w' &= w - H^{-1} \\nabla C \\\\\n\\end{align*}\n\\]\nAdvantages:\n\nconverges faster than 1st order approximation\nversions of backprop exist to calculate the Hessian\n\nDisadvantage: Too expensive and space heavy\n\n\n\nThe Hessian adds information about the way the gradient is changing. The momentum based methods aims to do this by augmenting with a velocity term:\n\\[\n\\begin{align*}\n  v \\to v' &= \\mu v - \\eta \\nabla C \\\\\n  w \\to w' &= w + v' \\\\\n\\end{align*}\n\\]\n\\(\\mu\\) controls the amount of velocity in the system (momentum co-efficient, though intuitively it controls the friction). \\(\\mu = 1 \\implies\\) there is no friction, hence the system will roll down a hill and may overshoot. For \\(\\mu &gt; 0 \\implies\\) the friction is infinte, i.e. the system reduces to the original udpate equations.\nNote: for \\(\\mu &gt; 1\\), energy will be added to the system, which could result in instability. For \\(\\mu &lt; 0\\), gradient may traverse uphill instead.\n\n\n\n\nConjugate gradient descent\nBFGS / L-BFGS (the latter being the limited memory implentation)\nWhat about Barzilai-Borwein?\nPromising: Nesterov’s accelerated gradient technique (though it may be outdated now)\n\n\n\n\n\n\n\n\\[\n\\sigma(z) = \\frac{1}{2}(1 + \\tanh(z/2))\n\\]\nHence, it is simply a linear transformation. Note that the output of tanh is \\(\\in (-1, 1)\\), which can make interpretation slightly different. Potentially, tanh neurons can be better for learning because the output activation can be any sign. Consider BP4: \\(\\partial C / \\partial w = \\delta^l {a^{l-1}}^T\\); so for a sigmoid neuron, all the activations will be positive, which will mean that a whole row of weights \\(w_j^l\\) will increase / decrease depending on the sign of \\(\\delta_j^l\\), rather than increasing / decreasing independently of each other. Moreover, tanh is an odd function, hence the activations will on average be equally balanced between +ve and -ve.\n\n\n\nrectified linear unit\n\\[\n\\text{ReLu} = \\max(0, z)\n\\]\nReLu doesn’t suffer from the saturation problem. On the flip side, when \\(z &lt; 0\\), the gradient is 0, so the neuron stops learning entirely.\nNote: ReLu requires a different initialisation scheme. This article is very helpful introduction.",
    "crumbs": [
      "Notes",
      "Improving Learning"
    ]
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "NNs and Deep Learning",
    "section": "",
    "text": "This repo contains a portfolio of experiments and notes on NNs and DL. The aim is twofold:\n\nuse visualisations to understand the latent space when a NN is training\nkeep up to date with NN programming practices\n\n\n\n\nThe KL loss in a VAE encourages the encoder to map inputs to latents that are close to the prior distribution \\(\\mathcal{N}(0, I)\\). We can visualise this distribution for each class in the MNIST dataset by approximating it as a Mixture of Gaussians.\n\n\n\n\n\n\nStandard VAE (β = 1)\n\n\n\n\n\nThe class distributions converge to very different distributions. Digit 0 has a larger spread.\n\n\n\n\n\nβ = 10\n\n\n\n\n\n The distribution shapes are more similar, but they still try to converge to different locations. \n\n\n\n\n  Both VAEs have the same architecture with a 2D latent space, and were trained for a single epoch. In both cases, the model learns to try and separate the the location of the class distributions, however there is significant overlap between the numbers 4 and 9, which is to be expected. The shapes of the distributions are very similar in the beta VAE, which is due the stronger KL loss. \n\n\n\n\n\n\nIn classification tasks, a NN learns weights so that it is able to create simple decision boundaries to separate classes in the latent space.\n\n\n\n\n\n\nVisualising hidden layer with 3 nodes - each has its own axis (NN layers: {784,10,3,10}). As epoch increases, the learnt weights push each digit class to a corner. Unsurprisingly, digits 4 and 9 have significant overlap! See this folder for implementation.\n\n\n\n\n\n\nWhen there is no non-linearity in the NN, the weights are equivalent to a single linear transformation. In the case of classification, intuitively, we are applying a mask on the input.\n\n\n\n\n\n\nWeights learnt for each digit in a NN with no hidden layer. This is equivalent to applying a mask / linear transformation. See this notebook for implementation.\n\n\n\n\n\nREADME.md\nexperiments/  - NN PyTorch class, training experiments\nnotes/        - markdown notes for experiments and theory\nresources/    - store dataset, model and figures\n\n\n\nThe notes/ folder contains markdown notes on the relevant NN theory required for the experiments. It also contains notes and exercises from the book Neural Networks and Deep Learning.\nFor each chapter, I have written some notes and answers to most exercises / problems:\n\n1 Neural Network Intro\n2 Backpropagation\n3 Improving Learning\n\nThis is a WIP; I have yet to do the later chapters. I also aim to cover the following topics:\n\nNotes on Activation Functions\n\nSwish, softplus (for VAE to predict variance)\n\nRegularisation: L1, L2, Dropout, Continual Backprop\nGrid search over batch-size, lr using hydra\n\n\n\n\nThis documents the best practices I have learnt for programming NNs.\n\n\ndef train_loop(self, ...):\n  for epoch in range(epochs):\n    for batch in dataloader:\n      self.train()\n      # 1. Move data to device\n      batch = batch.to(device) \n      # 2. Zero gradients\n      self.optimizer.zero_grad()\n      # 3. Forward pass\n      output = self.model(batch)\n      # 4. Compute loss\n      loss = self.loss_function(output, batch)\n      # 5. Backward pass\n      loss.backward()\n      # 6. Update weights\n      self.optimizer.step()\n\n\n\nconfiguring NNs and storing (hyper)parameters\n@dataclass\nclass ModelConfig:\n  # model parameters\n  hidden_dim: int = 128\n  # training parameters\n  lr: float = 1e-3\n  batch_size: int = 32\n\n  def save_config(self, path: Path):\n    OmegaConf.save(OmegaConf.to_yaml(self), path)\n\n  @staticmethod\n  def load_config(path: Path) -&gt; \"ModelConfig\":\n    conf = OmegaConf.load(path)\n    return ModelConfig(**conf)\n\n\n\nfor logging: see base.py\nA list of potential avenues to explore:\n\npytorch-lightning\nhydra\nfire\n\n\n\n\nQuarto was used to generate the website for this repo.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "README.html#organisation",
    "href": "README.html#organisation",
    "title": "NNs and Deep Learning",
    "section": "",
    "text": "README.md\nexperiments/  - NN PyTorch class, training experiments\nnotes/        - markdown notes for experiments and theory\nresources/    - store dataset, model and figures",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "README.html#notes",
    "href": "README.html#notes",
    "title": "NNs and Deep Learning",
    "section": "",
    "text": "The notes/ folder contains markdown notes on the relevant NN theory required for the experiments. It also contains notes and exercises from the book Neural Networks and Deep Learning.\nFor each chapter, I have written some notes and answers to most exercises / problems:\n\n1 Neural Network Intro\n2 Backpropagation\n3 Improving Learning\n\nThis is a WIP; I have yet to do the later chapters. I also aim to cover the following topics:\n\nNotes on Activation Functions\n\nSwish, softplus (for VAE to predict variance)\n\nRegularisation: L1, L2, Dropout, Continual Backprop\nGrid search over batch-size, lr using hydra",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "README.html#framework-for-experiments",
    "href": "README.html#framework-for-experiments",
    "title": "NNs and Deep Learning",
    "section": "",
    "text": "This documents the best practices I have learnt for programming NNs.\n\n\ndef train_loop(self, ...):\n  for epoch in range(epochs):\n    for batch in dataloader:\n      self.train()\n      # 1. Move data to device\n      batch = batch.to(device) \n      # 2. Zero gradients\n      self.optimizer.zero_grad()\n      # 3. Forward pass\n      output = self.model(batch)\n      # 4. Compute loss\n      loss = self.loss_function(output, batch)\n      # 5. Backward pass\n      loss.backward()\n      # 6. Update weights\n      self.optimizer.step()\n\n\n\nconfiguring NNs and storing (hyper)parameters\n@dataclass\nclass ModelConfig:\n  # model parameters\n  hidden_dim: int = 128\n  # training parameters\n  lr: float = 1e-3\n  batch_size: int = 32\n\n  def save_config(self, path: Path):\n    OmegaConf.save(OmegaConf.to_yaml(self), path)\n\n  @staticmethod\n  def load_config(path: Path) -&gt; \"ModelConfig\":\n    conf = OmegaConf.load(path)\n    return ModelConfig(**conf)\n\n\n\nfor logging: see base.py\nA list of potential avenues to explore:\n\npytorch-lightning\nhydra\nfire\n\n\n\n\nQuarto was used to generate the website for this repo.",
    "crumbs": [
      "Introduction"
    ]
  }
]